%\documentclass[a4paper,superscriptaddress,11pt]{quantumarticle}
\documentclass[aps,twocolumn,longbibliography,english,superscriptaddress]{revtex4-1}
%\documentclass[a4paper,superscriptaddress,11pt]{article}
\pdfoutput=1
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{upquote}
%\usepackage{multicol}
%\usepackage{caption}
%\usepackage[plain]{algorithm}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{rotating}
%\usepackage{cite}
\usepackage{booktabs}
%\usepackage{unicode-math}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithm
%\usepackage{algpseudocode}% http://ctan.org/pkg/algpseudocode
\usepackage{xcolor}% http://ctan.org/pkg/xcolor
\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother

\usepackage{bbm}
\usepackage{jlcode}
\usepackage{graphicx, subfigure}
\usepackage{amsmath,color,amsthm}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage[normalem]{ulem}
\usepackage{indentfirst}
\usepackage{txfonts}
\usepackage[epsilon, tsrm, altpo]{backnaur}

\usepackage{listings}
\lstset{
    language=Julia,
    basicstyle=\ttfamily\footnotesize,
    numberstyle=\scriptsize,
    % numbers=left,
    backgroundcolor=\color{gray!10},
    frame=single,
    tabsize=2,
    rulecolor=\color{black!30},
    title=\lstname,
    %escapeinside={\%(*}{*)},
    breaklines=true,
    breakatwhitespace=true,
    framextopmargin=2pt,
    framexbottommargin=2pt,
    extendedchars=true,
    inputencoding=utf8,
    columns=fullflexible,
}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=1000
\hbadness=1000

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

%Journal reference.  Comma sets off: name, vol, page, year
\def\journal #1, #2, #3, 1#4#5#6{{\sl #1~}{\bf #2}, #3 (1#4#5#6) }
\def\pr{\journal Phys. Rev., }
\def\prb{\journal Phys. Rev. B, }
\def\prl{\journal Phys. Rev. Lett., }
\def\pl{\journal Phys. Lett., }
%\def\np{\journal Nucl. Phys., }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\usepackage{CJK}
%\usepackage[colorlinks, citecolor=blue]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}

%%%%%% Shortcut related
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\newcommand{\out}{{\vx^L}}
\newcommand{\inp}{{\vx^0}}
\newcommand{\cquad}{{{ }_{\quad}}}
\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\minuseq}{\mathrel{-}=}
\newcommand{\vx}{{\mathbf{x}}}
\newcommand{\vg}{{\mathbf{g}}}
\newcommand{\vp}{{\mathbf{p}}}
\newcommand{\vy}{{\mathbf{y}}}
\newcommand{\vvalue}{{\texttt{value}}}
\newcommand{\grad}{{\texttt{grad}}}
\newcommand{\parameter}{{\texttt{parameter}}}
%%%%%% Convention related
\newcommand{\SWAP}{{\rm SWAP}}
\newcommand{\CNOT}{{\rm CNOT}}
\newcommand{\X}{{\rm X}}
\renewcommand{\H}{{\rm H}}
\newcommand{\Rx}{{\rm Rx}}
\renewcommand{\v}[1]{{\bf #1}}
\newcommand{\dataset}{{\mathcal{D}}}
\newcommand{\wfunc}{{\psi}}
\newcommand{\SU}{{\rm SU}}
\newcommand{\UU}{{\rm U}}
\newcommand{\thetav}{{\boldsymbol{\theta}}}
\newcommand{\gammav}{{\boldsymbol{\gamma}}}
\newcommand{\thetai}{{\theta^\alpha_l}}
\newcommand{\Expect}{{\mathbb{E}}}
\newcommand{\Tr}{{\rm Tr}}
\newcommand{\etc}{{\it etc~}}
\newcommand{\etal}{{\it etal~}}
\newcommand{\xset}{\mathbf{X}}
\newcommand{\fl}{\texttt{fl}}
\newcommand{\pdata}{\mathbf{\pi}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\epdata}{\mathbf{\hat{\pi}}}
\newcommand{\gammaset}{\boldsymbol{\Gamma}}
\newcommand{\ei}{{\mathbf{e}_l^\alpha}}
\newcommand{\vtheta}{{\boldsymbol{\theta}}}
\newcommand{\sigmag}{{\nu}}
\newcommand{\sigmai}[2]{{\sigma^{#2}_{#1}}}
\newcommand{\qi}[1]{{q^{\alpha_{#1}}_{#1}}}
\newcommand{\BAS}{Bars-and-Stripes}
\newcommand{\circled}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}

\newcommand{\qexpect}[1]{{\left\langle #1\right\rangle}}
\newcommand{\expect}[2]{{\mathop{\mathbb{E}}\limits_{\substack{#2}}\left[#1\right]}}
\newcommand{\var}[2]{{\mathop{\mathrm{Var}}\limits_{\substack{#2}}\left(#1\right)}}
\newcommand{\pshift}[1]{{p_{\thetav+#1}}}
\newcommand{\upcite}[1]{\textsuperscript{\cite{#1}}}
\newcommand{\Eq}[1]{Eq.~(\ref{#1})}
\newcommand{\Fig}[1]{Fig.~\ref{#1}}
\newcommand{\Tbl}[1]{Table~\ref{#1}}
\newcommand{\Sec}[1]{Sec.~\ref{#1}}
\newcommand{\App}[1]{Appendix \ref{#1}}
\newcommand{\bra}[1]{\mbox{$\left\langle #1 \right|$}}
\newcommand{\ket}[1]{\mbox{$\left| #1 \right\rangle$}}
\newcommand{\braket}[2]{\mbox{$\left\langle #1 | #2 \right\rangle$}}
\newcommand{\tr}[1]{\mathrm{tr}\mbox{$\left[ #1\right]$}}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

%%%%%% Comment related
\newcommand{\red}[1]{[{\bf  \color{red}{LW: #1}}]}
\newcommand{\xred}[1]{[{\bf  \color{red}{\sout{LW: #1}}}]}
\newcommand{\blue}[1]{[{\bf  \color{blue}{JG: #1}}]}
\newcommand{\green}[1]{[{\bf  \color{green}{TZ: #1}}]}
\newcommand{\xgreen}[1]{[{\bf  \color{green}{\sout{TZ: #1}}}]}
\newcommand{\xblue}[1]{[{\bf  \color{blue}{\sout{JG: #1}}}]}
\newcommand{\material}[1]{\iffalse[{\bf  \color{cyan}{Material: #1}}]\fi}

\newtheorem{theorem}{\textit{Theorem}}
\theoremstyle{definition}\newtheorem{definition}{\textit{Definition}}

\makeatother

\begin{document}
\title{Instruction level automatic differentiation on a reversible Turing machine}

%\author{Jin-Guo Liu\thanks{cacate0129@iphy.ac.cn}\\
%Institute of Physics, Chinese Academy of Sciences,\\Beijing 100190, China\\
%\And
%Hong-Xuan Zhao-Wang\\
%Department of Computer Science, University of Tsukuba
%}
%\author{Lei Wang}
%\email{wanglei@iphy.ac.cn}
%\affiliation{Institute of Physics, Chinese Academy of Sciences, Beijing 100190, China}
%\affiliation{CAS Center for Excellence in Topological Quantum Computation, University of Chinese Academy of Sciences, Beijing 100190, China}
%\affiliation{Songshan Lake Materials Laboratory, Dongguan, Guangdong 523808, China}

\author{Jin-Guo Liu}
\email{cacate0129@iphy.ac.cn}
\affiliation{Institute of Physics, Chinese Academy of Sciences, Beijing 100190, China}

\author{Taine Zhao}
\affiliation{Department of Computer Science, University of Tsukuba}

\begin{abstract}
    This paper considers source to source automatic differentiation (AD) on a reversible Turing machine. We start by reviewing why adjoint mode AD is hard for traditional machine learning frameworks and propose a solution to existing issues by writing a program reversiblly. We developed an reversible eDSL NiLang in Julia that can be used to generate backward rules. We demonstrate its power by differentiating over the reversible implementations of Bessel function, and linear algebra functions unitary matrix multiplication and QR decomposition. It is also a promising direction towards solving the notorious memory wall problem in machine learning. We also discuss the challenges that we face towards rigorous reversible programming from the instruction and hardware perspective.
\end{abstract}


\maketitle

%\begin{multicols}{2}
\section{Introduction}\label{sec:intro}
    \blue{this is a comment!}

    Computing the gradients of a numeric model $f : \mathbb{R}^m \rightarrow \mathbb{R}^n$ plays a crutial role in scientific computing. Consider a computing process
\begin{align*}
    &\vx^1 = f_1(\vx^0)\\
    &\vx^2 = f_2(\vx^1)\\
    &\ldots\\
    &\vx^L = f_L(\vx^{L-1})
\end{align*}
where $x^0\in R^m$, $x^L\in R^n$, $L$ is the depth of computing.
The Jacobian of this program is a $n\times m$ matrix $J_{ij} \equiv \frac{\partial x^L_i}{\partial x_j^0}$, where $x_j^0$ and $x_i^L$ are single elements of inputs and outputs.
Computing part of the Jacobian automatically is what we called automatic differentiation (AD). It can be classified into three classes, the tangent mode AD, the adjoint mode AD and the mixed mode AD.~\cite{Hascoet2013}
The tangent mode AD computes the Jacobian matrix elements that related to a single input using the chain rule $\frac{\partial \vx^k}{\partial x^0_j} = \frac{\partial \vx^k}{\partial \vx^{k-1}}\frac{\partial \vx^{k-1}}{\partial x^0_j}$, while a tangent mode AD computes Jacobian matrix elements that related to a single output using $\frac{\partial \vx^k}{\partial x^0_j} = \frac{\partial \vx^k}{\partial \vx^{k-1}}\frac{\partial \vx^{k-1}}{\partial x^0_j}$. Mixed mode AD is a mixture of both.
    In variational applications where the loss function always outputs a scalar, the adjoint mode AD is perfered,.
However, implementing adjoint mode AD is harder than implementing its tangent mode counterpart, because it requires propagating the gradients in the inverse direction of computing the loss. The back propagation of gradients requires intermediate information of a program that includes
\begin{enumerate}
    \item the computational process,
    \item and variables used in computing gradients.
\end{enumerate}
    The computational process is often stored in a computational graph, a directed acyclic graph (DAG) that represents the relationship between data and functions. In Pytorch~\cite{Paszke2017} and Flux~\cite{Innes2018a}, every variable has a tracker field that stores its parent information, i.e., the input data and function generating this variable. TensorFlow~\cite{Tensorflow2015} implements a static computational graph as a description of the program before actual computation happens. The required variables are also recorded in this graph.
    For source to source AD package, Tapenade~\cite{Hascoet2013} uses source code as the computational graph and Zygote~\cite{Innes2018, Innes2019} uses an intermediate representation (IR) of a program, the static single assignment (SSA) form, as the computational graph. To cache intermediate states, they use a global stack.

    Several limitations are observed in these AD implementations due to the recording and caching. First of all, most packages require a lot of primitive functions with programmer-defined backward rules. For example, the backward rule of Bessel functions must be provided although it is composed of basic instructions `+', `-', `*', `/', and conditional jumps. Defining backward rules for these basic instructions in the computational graph scheme suffers from the overhead of memorizing the computational graph and caching intermediate states. Even in Tapenade, the program has to remember the control at each place where the flow merges in the forward sweep. % It can decrease the performance for more than two orders when a program contains loops (as we will show later).
    Secondly, the memory consumption is significant, also known as the memory wall problem.~\cite{memorywall}. The overhead of naive caching every input of instructions is linear to the computing time. In many deep learning models like recurrent neural network~\cite{Lipton2015} and residual neural networks~\cite{He2016}, the depth can reach several thousand, where the memory is often the bottleneck of these programs. Another important source of memory overhead is from the fact that inplace functions are forbidden delibrately in a computational graph based AD scheme in order to protect the cached data.
    Thirdly, obtaining higher-order gradients are not efficient in most of these packages. For example, in most machine learning packages, people back-propagate the whole program of obtaining first-order gradients to obtain second-order gradients. The repeated use of back-propagation algorithm causes an exponential overhead concerning the order of gradients. A better approach is using Taylor propagation like in JAX~\cite{Bettencourt2019} and beautiful differentiation~\cite{Elliott2009}. However, Taylor propagation in the adjoint mode AD requires tedious implemention of higher order backward rules for primitives.

% where the manually derived backwards rule still faces the degenerate spectrum problem (gradients explodes), instruction-level AD will return reasonable gradients. With instruction-level AD, people don't worry about inplace functions, which may be a huge problem in traditional approaches. We can back-propagate over a quantum simulator, where all instructions are reversible two level unitaries (i.e. Jacobian rotation).

%We don't need extra effort to learn meta parameters.~\cite{} Neural ODE is much easier to design~\cite{Chen2018}.

    We tackle these issues by making a program reversible. In the machine learning field, reversibility has been used in reduce the memory allocations in recurrent neural networks~\cite{MacKay2018} and residual neural networks~\cite{Behrmann2018}. These techinics include information buffer~\cite{Maclaurin2015} and reversible activation functions~\cite{Gomez2017,Jacobsen2018}. Our approach is general purposed. We develop an embedded domain-specific language (eDSL) NiLang in Julia language~\cite{Bezanson2012,Bezanson2017} that implements reversible programming.~\cite{Perumalla2013,Frank2017}.
    This eDSL provides a macro to generate reversible functions, and is completely compatible with Julia ecosystem. One can write reversible control flows, instructions and memory managements in this macro. Combining it with Julia's type system, we implement the AD engine within 100 lines that differentiate any program written in this eDSL, including linear algebra functions.

    In history, there have been some prototypes of reversible languages like Janus~\cite{Lutz1986}, R (not the popular one)~\cite{Frank1997}, Erlang~\cite{Lanese2018} and object-oriented ROOPL~\cite{Haulund2017}. % These languages have reversible control flow that allows user to input an additional postcondition in control flows to help programs run backward.
    In the past, the primary motivation of making a program reversible is to support energy efficient reversible computing devices~\cite{Frank1999} like adiabatic complementary metal–oxide–semiconductor (CMOS)~\cite{Koller1992}, molecular mechanical computing system~\cite{Merkle2018} and superconducting system~\cite{Likharev1977,Semenov2003}. These devices either implements reversible logical gates or is able to recover signal energy, where the latter is also called generalized reversible computing. Both schemes do not have a lower bound of energy consumption from information and entropy perspective, which is known as the Landauer's principle~\cite{Landauer1961}
    %They do not erase information hence do not have a lower bound of energy consumption by Landauer principle~\cite{Landauer1961}. However, investigators show less interest to reversible programming since 15 years ago, because the energy efficiency of traditional CMOS devices is still several orders~\cite{Debenedictis2017,Frank2017} above this lower bound, removing this lower bound is not an urgent problem yet.
    After decades of efforts, reversible computing devices are very close to providing productivity now. For exmaple, adiabatic CMOS is more energy efficient than a traiditonal CMOS and can be used in a spacecraft~\cite{Debenedictis2017}, where energy is more valuable than device itself.
    From the software engineering perspective, reversible programming is a powerful tool to schedule asynchronious events~\cite{Jefferson1985} and debug a program bidirectionally~\cite{Boothe2000}.
    These applications are interesting on them own, but not appealing enough to motivate the reversible software ecosystem.
    Our work aims to breaks the information barrier between the machine learning community and the reversible programming community, and provides yet another strong motivation to develop reversible programming.

    In this paper, we first introduce the language design of NiLang in \Sec{sec:lang}.
    In \Sec{sec:bp}, we explain the back-propagation algorithm of Jacobians and Hessians in this eDSL.
    In \Sec{sec:example}, we show several examples including Fobonacci number, Bessel function, unitary matrix multiplication and QR decomposition~\cite{Golub2012}. We show how to generate first order and second order backward rules for these functions.
    In \Sec{sec:discussion}, we discuss several important issues, how time-space tradeoff works, reversible instructions and hardware, and finally, an outlook to some open problems to be solved.
    In the appendix, we show the grammar of NiLang and a gradient free self-consistent training strategy.

\section{Language design}\label{sec:lang}

\subsection{Intruductions to reversible language design}
    In a modern programming language, functions are pushed to a global stack for scheduling. The memory layout of a function consists of input arguments, a function frame with information like the return address and saved memory segments, local variables, and working stack. After the call, the function clears these runtime information, only stores the return value. In the reversible programming style, this kind of design pattern is no longer the best practice. One can not discard input variables and local variables easily after a function call, since discarding information may ruin reversibility. For this reason, reversible functions are very different from irreversible ones from multiple perspectives.

    First of all, the memory management in a reversible language is different.
    The key difference is when a variable in a reversible program is discarded, its contents should be known.
    We denote the allocation of a zero emptied memory to variable \texttt{x} as \texttt{x $\leftarrow$ 0}, and deallocate a zero emptied variable \texttt{x} as \texttt{x $\rightarrow$ 0}.
    A variable allocated and deallocated in a local scope is called an ancilla, it does not occupy the memory for long period.
    A variable can also be pushed to a stack and used later with a pop statement. This is similar to a traditional stack operation except it zero-clears the variable after pushing and presupposes the variable being zero-cleared before poping.

\begin{figure}
    \centerline{\includegraphics[width=0.8\columnwidth,trim={0 0cm 0 0cm},clip]{images/controlflow.pdf}}
    \caption{Flow chart for reversible (a) \texttt{if} statement and (b) \texttt{while} statement. ``stmts'', ``stmts1'' and ``stmts2'' are statements, statements in true branch and statements in false branch respectively. ``pre'' and ``post'' are precondition and postconditions respectively.}\label{fig:controlflow}
\end{figure}
Secondly, a reversible control flow is sepcially designed.
The reversible \texttt{if} statement is shown in \Fig{fig:controlflow} (a), the program enters the branch specified by precondition. After executing that branch, the program checks the consistency of precondition and postcondition to make sure they are the same. In the reverse pass, the program enters the branch specified by the postcondition.
For the reversible \texttt{while} statement shown in \Fig{fig:controlflow} (b), before executing the condition expressions, the program preassumes the postcondition is false.
After each iteration, the program asserts the postcondition to be true. In the reverse pass, we exchange the precondition and postcondition.
The reversible \texttt{for} statement is similar to irreversible ones except after executing the loop, the program checks the values of these variables to make sure they are not changed. In the reverse pass, we exchange start and stop and inverse the sign of step.

Lastly, the reversible arithmetic and boolean instructions are also different. Every instruction has a unique inverse that can undo the changes.
For logical expressions, we have \texttt{y $\veebar$= f(args...)} self reversible.
In the following discussion, we assume \texttt{y += f(args...)} and \texttt{y -= f(args...)} reverse to each other although it is not true for floating point numbers considering rounding error. Here \texttt{f} can be \texttt{identity}, \texttt{*}, \texttt{/} and \texttt{\^} et. al.
We will discuss the number system in detail later in \Sec{sec:hardware}

\subsection{NiLang's Reversible IR}
In the last subsection, we have reviewed basic building blocks of a typical reversible language. In order to insert the code of obtaining gradients into the reversed program, the reversible language design should have related abstraction power.
This motivates us to design a new reversible language NiLang to fit this task.
NiLang is an eDSL in Julia. We choose Julia as the host language for multipile purposes. Julia's meta programming and its package for pattern matching MLStyle.jl~\cite{MLStyle} allow us to define an eDSL conveniently. Meanwhile, the type inference and just in time compiling can remove most overheads introduced in our eDSL, providing a reasonable performance. Most importantly, the multiple dispatch provides the polymorphism that will be used in our autodiff engine.

The main feature of NiLang is contained in a single macro \texttt{@i} that compiles a reversible function.
The allowed statements in this eDSL are shown in \App{app:grammar}.
The following is a minimal example of compiling a NiLang function to native julia function.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> using NiLangCore, MacroTools

julia> macroexpand(Main, :(@i function f(x, y)
           SWAP(x, y)
       end)) |> MacroTools.prettify
quote
    $(Expr(:meta, :doc))
    function $(Expr(:where, :(f(x, y))))
        gaur = SWAP(x, y)
        x = (NiLangCore.wrap_tuple(gaur))[1]
        y = (NiLangCore.wrap_tuple(gaur))[2]
        return (x, y)
    end
    if typeof(f) != typeof(~f)
        function $(Expr(:where, :((  #$ TODO: remove this comment
                    mongoose::typeof(~f))(x, y))))
            mandrill = (~SWAP)(x, y)
            x = (NiLangCore.wrap_tuple(mandrill))[1]
            y = (NiLangCore.wrap_tuple(mandrill))[2]
            return (x, y)
        end
    end
    if !(NiLangCore._hasmethod1(
                NiLangCore.isreversible, typeof(f)))
        NiLangCore.isreversible(::typeof(f)) = true
    end
end
\end{lstlisting}
\end{minipage}

Macro \texttt{@i} generates three functions \texttt{f}, \texttt{$\sim$f} and \texttt{NiLangCore.isreversible}. \texttt{f} and \texttt{$\sim$f} are a pair of functions that reverse to each other, where \texttt{$\sim$f} is an callable of type \texttt{Inv\{typeof(f)\}}.
In the body of \texttt{f}, \texttt{NiLangCore.wrap\_tuple} is used to unify output data types, it will wrap any non-tuple variable to a tuple.
The outputs of \texttt{SWAP} are assigned back to its input variables, in other words, a function modifies inputs inplace.
At the end this function, this macro attaches a return statement that returns all input variables.
\texttt{NiLangCore.isreversible} is a function to mark the reversibility trait of \texttt{f}.
%The generated functions will be compiled to reversible ``instructions'', these instructions are closed under the inverse operation ``$\sim$''. Hence all functions defined in NiLang are also closed under ``$\sim$'' operations.

To understand the design of reversibility, we first introduce a reversible IR that plays a central role in NiLang.
In this IR, a statement can be an instruction, a function call, a control flow, a memory allocation/deallocation, or the inverse statement ``$\sim$''.
Any statement is this IR has a unique inverse as shown in \Tbl{tbl:revstatements}.

\begin{table}[h!]\centering
\begin{minipage}{\columnwidth}
\ra{1.3}
    \scalebox{1.0}{
        \begin{tabularx}{\textwidth}{X X}\toprule
            \textbf{statement} & \textbf{inverse}\\
            \hline
            \texttt{<f>(<args>...)} & \texttt{($\sim$<f>)(<args>...)}\\
            \hline
            \texttt{<y> += <f>(<args>...)} & \texttt{<y> -= <f>(<args>...)}\\
            \hline
            \texttt{<y> .+= <f>.(<args>...)} & \texttt{<y> .-= <f>.(<args>...)}\\
            \hline
            \texttt{<y> $\veebar$= <f>(<args>...)} & \texttt{<y> $\veebar$= <f>(<args>...)}\\
            \hline
            \texttt{<y> .$\veebar$= <f>(<args>...)} & \texttt{<y> .$\veebar$= <f>(<args>...)}\\
            \hline
            \texttt{<a> $\leftarrow$ <expr>} & \texttt{<a> $\rightarrow$ <expr>}\\
            \hline
            \texttt{(<T1> => <T2>)(<x>)} & \texttt{(<T2> => <T1>)(<x>)}\\
            \hline
            \texttt{begin}\newline \texttt{$\cquad$<stmts>}\newline \texttt{end} & \texttt{begin}\newline \texttt{$\cquad$ $\sim$(<stmts>)}\newline \texttt{end}\\
            \hline
            \texttt{if (<pre>, <post>)}\linebreak \texttt{$\cquad$<stmts1>}\newline \texttt{else}\newline \texttt{$\cquad$<stmts2>}\newline \texttt{end} & \texttt{if (<post>, <pre>)}\newline \texttt{$\cquad$$\sim$(<stmts1>)}\newline \texttt{else}\newline \texttt{$\cquad$ $\sim$(<stmts2>)}\newline \texttt{end}\\
            \hline
            \texttt{while (<pre>, <post>)}\newline \texttt{$\cquad$<stmts>} \newline \texttt{end} & \texttt{while (<post>, <pre>)}\newline $\cquad$  \texttt{$\sim$(<stmts>)}\newline \texttt{end}\\
            \hline
            \texttt{for <i>=<m>:<s>:<n>}\newline $\cquad$\texttt{<stmts>}\newline \texttt{end} & \texttt{for <i>=<m>:-<s>:<n>}\newline $\cquad$ \texttt{$\sim$(<stmts>)}\newline \texttt{end}\\
            \hline
            \texttt{@safe <expr>} & \texttt{@safe <expr>}\\
            \bottomrule
        \end{tabularx}
    }
    \caption{A collection of reversible statements.
    ``.'' is the symbol for broadcasting magic in Julia,
    ``$\sim$'' is the symbol for reversing a statement or a function.
    \texttt{<...>} represents a non-keyword, 
    where \texttt{<pre>} stands for precondition, 
    \texttt{<post>} stands for postcondition,
    \texttt{<args>...} stands for the argument list of a function,
    \texttt{<stmts>} stands for statement,
    \texttt{<exprs>} stands for expression,
    \texttt{<T1>} and \texttt{<T2>} stand for types and the reset are variables.
}\label{tbl:revstatements},
\end{minipage}
\end{table}

%For example, an instruction \texttt{y $\mathrel{+}=$ f(args...)} is interpreted as a Julia function call \texttt{PlusEq(f)(y, args...)} or \texttt{$\oplus$(f)(y, args...)} as a shorthand.
%Here, the ``!'' after a variable is used as a convension to indicate that a variable is changed after the call of an instruction or function.
%The detailed specification of instructions is listed in \App{app:instr}. The function call is same as the host language, except every function \texttt{f} has a \texttt{$\sim$f} that binded to an object of type \texttt{Inv\{typeof(f)\}}. \texttt{$\sim$f} invokes the compiled inverse functions of \texttt{f}.

%This design allows users putting additional postcondition in control flows to help reverse the program.
%A postcondition is a boolean expression that being evaluated after the body expressions being executed.
``\texttt{$\leftarrow$}'' and ``\texttt{$\rightarrow$}'' are symbols for memory allocation and deallocation, one can input them by typing ``$\backslash$leftarrow'' and ``$\backslash$rightarrow'' respectively followed by a Tab key in a Julia editor or REPL.
``\texttt{begin <stmts> end}'' is the block statement in Julia, it represents a code block.
It can be inverted by reversing the order of \texttt{<stmts>} as well as each element in it.
The conditional expression in \texttt{if} or \texttt{while} statements is a tuple of precondition and postcondition.
%\texttt{<a> $\leftarrow$ <expr>} binds variable \texttt{<a>} to an initial value specified by \texttt{<expr>}. Its inverse \texttt{<a> $\rightarrow$ <expr>} deallocates the variable \texttt{<a>}. Before deallocating the variable, the program checks that the its value is the same as the value of \texttt{<expr>}, otherwise throws an \texttt{InvertibilityError}.
%A function call in NiLang supports Julia's broadcasting magic, a function defined on scalars can be broadcasted to vectors by adding a ``.'' after the function call.
%\texttt{$\rightarrow$} will be added automatically.
    %The additional check underlines the difference between the irreversible assign statement and reversible ancilla statement.
Finally, the special macro \texttt{@safe} allows users to use external statements that do not break reversibility.
For example, one can use \texttt{@safe @show <var>} for debugging.

\subsection{Compiling}
The compilation of a reversible function contains three stages.

The first stage preprocess human inputs to a reversible IR.
The preprocessor expands the symbol ``\texttt{$\sim$}'' in the postcondition field of \texttt{if} statement by copying the precondition, adds missing ancilla ``$\leftarrow$'' statements to ensure ``\texttt{$\leftarrow$}'' and ``\texttt{$\rightarrow$}'' appear in pairs inside a function, a \texttt{while} statement or a \texttt{for} statement, and expands the uncomputing macro \texttt{$\sim$@routine}.
Since the "compmute-copy-uncompute" design pattern is extensively used in reversible programming for uncomputing ancillas.
One can use \texttt{@routine <stmt>} statement to record a statement, and \texttt{$\sim$@routine} to insert \texttt{$\sim$<stmt>} for uncomputing. 
The following example preprocesses an \texttt{if} statement to the reversible IR.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> using NiLangCore, MacroTools

julia> MacroTools.prettify(
       @code_preprocess if (x > 3, ~)
           @routine z += x * y
           ~@routine
       end)
:(if (x > 3, x > 3)
      z += x * y
      z -= x * y
  else
  end)
\end{lstlisting}
\end{minipage}

In this example, since the precondition ``\texttt{x > 3}'' is not change after execution of the specific branch, we omit the postcondition by putting a ``$\sim$'' in this field. ``\texttt{@routine}'' records a statement, the statement can also be a ``\texttt{begin <stmts> end}'' block as a senquence of statements.

The second stage generates the reversed code according to table \Tbl{tbl:revstatements}. For example,

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> MacroTools.prettify(
       @code_reverse if (pre, post)
           z += x * y
       else
           z += x / y
       end)
:(if (post, pre)
      z -= x * y
  else
      z -= x / y
  end)
\end{lstlisting}
\end{minipage}

The third stage is translating this IR and its inverse to native Julia code. It explains all functions as inplace and insert codes about reversibility check. At the end of a function definition, it attaches a return statement that returns all input arguments.
After this, the function is ready to execute on the host language.
The following example shows how an \texttt{if} statement is transformed in this stage.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> MacroTools.prettify(
       @code_interpret if (pre, post)
           z += x * y
       else
           z += x / y
       end)
quote
    bat = pre
    if bat
        @assignback (PlusEq(*))(z, x, y)
    else
        @assignback (PlusEq(/))(z, x, y)
    end
    @invcheck post bat
end
\end{lstlisting}
\end{minipage}

The compiler translates the instruction according to \Tbl{tbl:interp} and adds \texttt{@assignback} before each instruction and function call statement. The macro \texttt{@assignback} assigns the output of a function back to the arguments of that function. \texttt{@invcheck post bat} checks the consistency between preconditions and postconditions to ensure reversibility. This statement will throw an \texttt{InvertibilityError} error if target variables \texttt{bat} and \texttt{post} are not ``equal'' to each other up to a certian torlerance.

\subsection{Types and Dataviews}
So far, the language design is not too different from a traditional reversible language. To implement the adjoint mode AD, we introduce types and dataviews.
The type that used in the reversible context is just a normal Julia type with an extra requirement of having reversible constructors.
The inverse of a constructor is called a ``destructor'', which unpacks data and deallocates derived fields.
Data packing is implemented by reinterpreting the \texttt{new} function in Julia. For example,

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
    x ← new{TX, TG}(x, g)
\end{lstlisting}
\end{minipage}

Here, the ``$\leftarrow$'' statement followed by a \texttt{new} function is treated specially that it deallocates \texttt{g}. This makes sense because the output of \texttt{new} keeps all information in input argument list.
Its inverse is

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
    x → new{TX, TG}(x, g)
\end{lstlisting}
\end{minipage}

It unpacks \texttt{x} and allocates a new ancilla \texttt{g}.
The following example shows how to define as reversible type \texttt{GVar}.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> using NiLangCore

julia> @i struct GVar{T,GT} <: IWrapper{T}
           x::T
           g::GT
           function GVar{T,GT}(x::T, g::GT) where
                                       {T,GT}
               new{T,GT}(x, g)
           end
           function GVar(x::T, g::GT) where {T,GT}
               new{T,GT}(x, g)
           end
           @i function GVar(x::T) where T
               g ← zero(x)
               x ← new{T,T}(x, g)
           end
           @i function GVar(x::AbstractArray)
               GVar.(x)
           end
       end

julia> GVar(0.5)
GVar{Float64,Float64}(0.5, 0.0)

julia> (~GVar)(GVar(0.5))
0.5

julia> (~GVar)(GVar([0.5, 0.6]))
2-element Array{Float64,1}:
 0.5
 0.6
\end{lstlisting}
\end{minipage}

This piece of code is copied from the autodiff submodule of NiLang. \texttt{GVar} is a type used to store gradient information of a variable.
Here, we put \texttt{@i} macro before both \texttt{struct} and \texttt{function} statements. The ones before functions mark reversible functions, while the one before \texttt{struct} keyword is used to handle the function scope issue. It moves \texttt{$\sim$GVar} functions outside of this type definition.

The reversible cast between two types can be defined conveniently with the macro \texttt{@icast}.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> @pure_wrapper A

julia> @icast A(x) => GVar(x, g) begin
           g ← zero(x)
           g += identity(1)
       end

julia> x = A(0.5)
A(0.5)

julia> @instr (A=>GVar)(x)
GVar{Float64,Float64}(0.5, 1.0)

julia> @instr (GVar=>A)(x)
A(0.5)
\end{lstlisting}
\end{minipage}

Here, we first define a simple reversible wrapper \texttt{A} using macro \texttt{@pure\_wrapper}, and then the cast rule between \texttt{A} type and \texttt{GVar} type.
The body of cast is a reversible mapping that transforms \texttt{x} to \texttt{(x, g)}. The compiler appends a default constructor \texttt{DVar(xx, gg)} at the end of program to instantiate a new object as the return value.
Its inverse that coverts an object of type \texttt{GVar} to type \texttt{A} is automatically generated by reversing the above statements.
%With the flexibility to use types, it is not necessary to use global stacks in our eDSL.

The fields of an object can be accessed and manipuated by dataviews. A dataview can be an object, a field of a dataview, an array element of a dataview, or a bijective mapping of a dataview.
Let us first consider the following example.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> arr = [GVar(3.0), GVar(1.0)]
2-element Array{GVar{Float64,Float64},1}:
 GVar{Float64,Float64}(3.0, 0.0)
 GVar{Float64,Float64}(1.0, 0.0)

julia> x, y = 1.0, 2.0
(1.0, 2.0)

julia> @instr -arr[2].g += x * y
2.0

julia> arr
2-element Array{GVar{Float64,Float64},1}:
 GVar{Float64,Float64}(3.0, 0.0) 
 GVar{Float64,Float64}(1.0, -2.0)
\end{lstlisting}
\end{minipage}

Here, both \texttt{-arr[2].g}, \texttt{x} and \texttt{y} are dataviews. In Julia language, the statement \texttt{-grad(arr[2]) += x * y} should throw a syntax error because the function call ``\texttt{-}'' can not be assigned, and \texttt{GVar} is an immutable type.
In our eDSL, we wish it works because a memory cell is assumed to be modifiable in our eDSL.
The secret of how it works lies in the macro \texttt{@assignback}, it translates the above statement to

\begin{minipage}{.44\textwidth}
    \begin{lstlisting}[numberstyle=\scriptsize\color{gray},numbers=left,numbersep=8pt]
res = (PlusEq(*))(-arr[2].g, x, y)
arr[2] = chfield(arr[2], Val(:g),
    chfield(arr[2].g, -, res[1]))
x = res[2]
y = res[3]
\end{lstlisting}
\end{minipage}

The first line \texttt{PlusEq(*)(-arr[3].g, x, y)} computes the output, which is a tuple of length $3$.
At lines 2-3, \texttt{chfield(x, Val\{:g\}, val)} modifies the \texttt{g} field of \texttt{x} and \texttt{chfield(x, -, res[1])} returns \texttt{-res[1]}. Here, modifying a field requires the default constructor of a type not overwritten.
The assignments in lines 4 and 5 are straightforward.

\section{Automatic differentiation}\label{sec:bp}

Local Jacobians and Hessians for basic instructions used in this section could be found in \App{app:jacobians}.

\subsection{First order gradient}\label{sec:jacobian}
Consider a computation $\vx^{i-1} = f_{i}^{-1}(\vx^{i})$ in a reversed program,
the adjoint mode AD propagates the Jacobians in the reversed direction like
\begin{align}
    \begin{split}
        J^{\out}_{\out'} &= \delta_{\out,\out'},\\
        J^{\out}_{\vx^{i-1}} &= J^{\out}_{\vx^i} J^{\vx^i}_{\vx^{i-1}},
    \end{split}\label{eq:jacobian}
\end{align}
where $\out$ represents the outputs of the program, $J^{\out}_{\vx^i} \equiv \frac{\partial \out}{\partial \vx^i}$ is the Jacobian to be propagated, and $J^{\vx^i}_{\vx^{i-1}}$ is the local Jacobian matrix. The Einstein's notation~\cite{Einsum} is used here so that the duplicated index $\vx^{i}$ is summed over.
%Tagent mode instruction-level automatic differentiation can be implemented easily in a irreversible language with dual numbers~\cite{Revels2016}.
%Here we focus on the adjoint mode.
\Eq{eq:jacobian} can be rewritten in the diagram of tensor networks~\cite{Orus2014} as shown in \Fig{fig:ad}.
\begin{figure}
    \centerline{\includegraphics[width=0.95\columnwidth,trim={0.5cm 1cm 0 1cm},clip]{images/ad.pdf}}
    \caption{Computional processes in the tensor network diagram, a big circle with three legs represents a Hessian, a small circle with two legs represents a Jacobian. Dangling edges and connected edges stands for unpaired and paired labels respectively in the Einstein's notation. From left to right, the diagrams represent computing, uncomputing, back propagating Jacobians and back propagating Hessians.}\label{fig:ad}
\end{figure}

The algorithm to compute the adjoint mode AD can be summarized as follows.

\begin{algorithm}[H]
    \KwResult{\grad.($\vx_g$)}
    let \texttt{iloss} be the index of loss variable in $\vx$\\
    $\vy = f(\vx)$\\
    $\vy_g$ = \texttt{GVar}.($\vy$)\\
    \grad($\vy_g$[\texttt{iloss}]) += 1.0\\
    $\vx_g= f^{-1}(\vy_g)$
    \caption{Reversible programming AD}\label{alg:ad}
\end{algorithm}

The program first computes the forward pass, and then wrap each output variable with \texttt{GVar}.
The constructor \texttt{GVar} attaches a zero gradient field to a variable. If an input varaible is an array, \texttt{GVar} will be broadcasted to each array element automatically.
The line \texttt{\grad($\vy_g$[\texttt{iloss}]) += 1.0} adds one to the gradient field of loss to initialize a single row of Jacobian as in the first line of \Eq{eq:jacobian}.
Finally, execute the inverse program \texttt{$f^{-1}$}, the gradients are stored in the $\grad$ dataview of output variables.
The computation of gradients are implemented with multiple dispatch, that is, when an instruction has a \texttt{GVar} type in its argument list, it calls a different routine. The same trick is used in the dual number implementation of tangent mode AD~\cite{Revels2016}.
%Its inverse \texttt{$\sim$GVar} deallocates the gradient field safely and returns its value field.  Here, "safely" means before deallocation, the program will check the gradient field to make sure its value is restored to $0$.
%When an instruction \texttt{instruct} meets a \texttt{GVar}, besides computing its value field $\vvalue(\vy) = \texttt{instruct}(\vvalue(\vx))$, it also updates the gradient field $\grad(\vy) = \left[J^{\vy}_{\vx}\right]^{-1} \grad(\vx)$, where $\left[J^{\vy}_{\vx}\right]^{-1}$ is the Jacobian of $\texttt{instruct}^{-1}$. One can define this gradient function on either \texttt{instruct} or \texttt{instruct${}^{-1}$}.
In the following, we examplify the design by binding the adjoint rules to instructions \texttt{$\oplus(*)$} and \texttt{$\ominus(*)$}

\begin{minipage}{.44\textwidth}
\begin{lstlisting}[mathescape=true]
@i function ⊖(*)(out!::GVar, x::GVar, y::GVar)
    value(out!) -= value(x) * value(y)
    grad(x) += grad(out!) * value(y)
    grad(y) += value(x) * grad(out!)
end
\end{lstlisting}
\end{minipage}

Here, we adopt a convension that only variables ended with \texttt{!} will be changed after the function call.
%This backward rule has been included in NiLang, one can check the gradients by typing in a Julia REPL
%If one defines the backward rule on \texttt{instruct}, the compiler will generate the backward rule for its inverse \texttt{instruct${}^{-1}$} as the inverse function. This is doable 
Although the backward rule is defined on $\ominus(*)$, the compiler generates the backward rules on $\oplus(*)$ too.
This reflects the fact that taking inverse and computing gradients commute to each other~\cite{Mcinerney2015}. Hence for a general reversible function $f$, one can bind backward rule on either $f$ or its inverse $f^{-1}$.
%When gradients are not used anymore, the reversible way to deallocate gradients is uncomputing the whole process of obtaining them.
%, which increases the hyrachy by 1. Whenever the hyrachy increase by 1, the computational overhead doubles comparing with its irreversible counter part.
We can check the correctness of our definition like follows.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}[mathescape=true]
julia> using NiLang, NiLang.AD

julia> a, b, y = GVar(0.5), GVar(0.6), GVar(0.9)
(GVar(0.5, 0.0), GVar(0.6, 0.0), GVar(0.9, 0.0))

julia> @instr grad(y) += identity(1.0)

julia> @instr y += a * b
GVar(0.6, -0.5)

julia> a, b, y
(GVar(0.5, -0.6), GVar(0.6, -0.5), GVar(1.2, 1.0))

julia> @instr y -= a * b
GVar(0.6, 0.0)

julia> a, b, y
(GVar(0.5, 0.0), GVar(0.6, 0.0), GVar(0.899999, 1.0))
\end{lstlisting}
\end{minipage}

Here, $J(\ominus(*)) = J(\oplus(*))^{-1}$, hence consecutively applying them restores gradient fields of variables.
The implementation of Algorithm \ref{alg:ad} is so short that we present the function definition as follows.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
@i function (g::Grad)(args...; kwargs...)
    @safe @assert count(x -> x isa Loss, args) == 1
    iloss ← 0
    @routine for i=1:length(args)
        if (tget(args,i) isa Loss, iloss==i)
            iloss += identity(i)
            (~Loss)(tget(args,i))
        end
    end

    g.f(args...; kwargs...)
    GVar.(args)
    grad(tget(args,iloss)) += identity(1.0)
    (~g.f)(args...; kwargs...)

    ~@routine
end
\end{lstlisting}\label{lst:ad}
\end{minipage}

The program first checks variables contain exactly one \texttt{Loss}, where \texttt{Loss} is a reversible wrapper used to mark the loss variable. 
Then we locates the loss variable as \texttt{iloss} and use \texttt{$\sim$Loss} unwraps the loss variable. After computing the forward pass and backward pass, \texttt{$\sim$@routine} uncomputes the ancilla \texttt{iloss} and returns the location information to the loss variable.
Here, \texttt{tget(args, i)} returns the $i$-th element of a tuple. We forbid tuple indexing deliberately in order to avoid possible ambiguity in supporting array indexing.

The overhead of using \texttt{GVar} type is negligible thanks to Julia's multiple-dispatch and type inference. Let us consider a simple example that accumulates $1.0$ to a target variable $x$ for $n$ times.

\blue{Grammarly here!}

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> using NiLang, NiLang.AD, BenchmarkTools

julia> @i function prog(x, one, n::Int)
           for i=1:n
               x += identity(one)
           end
       end

julia> @benchmark prog'(Loss(0.0), 1.0, 10000)
BenchmarkTools.Trial: 
  memory estimate:  1.05 KiB
  allocs estimate:  39
  --------------
  minimum time:     35.838 μs (0.00% GC)
  median time:      36.055 μs (0.00% GC)
  mean time:        36.483 μs (0.00% GC)
  maximum time:     185.973 μs (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     1
\end{lstlisting}
\end{minipage}

\begin{figure}
    \centerline{\includegraphics[width=0.9\columnwidth,trim={0 0cm 0 0},clip]{images/fig3.pdf}}
    \caption{The time to obtain gradient as function of loop size. $\times n$ in lengend represents a rescaling of time.}\label{fig:benchmark}
\end{figure}
We implement the same function with TensorFlow, PyTorch and Zygote for comparison. The code could be found in our paper's github repository~\cite{benchmark}. Benchmark results on CPU Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz are shown in \Fig{fig:benchmark}.
One can see that the NiLang implementation is unreasonably fast, it is approximately two times the forward pass written in native Julia code.
Reversible programming is not always as fast as its irreversible counterparts. In practical applications, a reversible program may have memory or computation overhead. We will discuss the details of time and space trade off in \Sec{sec:timespace}.

\subsection{Second-order gradient}
Second-order gradients can be obtained in two different approaches.
\subsubsection{Back propagating first-order gradients}\label{sec:simplehessian}
Back propagating the first-order gradients is the most widely used approach to obtain the second-order gradients. Suppose the function space is closed under gradient operation, one can obtain higher-order gradients by recursively differentiating lower order gradient functions without defining new backward rules.
\begin{figure}[h]
    \centerline{\includegraphics[width=\columnwidth,trim={0 1cm 0cm 1cm},clip]{images/simplehessian.pdf}}
    \caption{Data flow in obtaining the second-order gradient with the recursive differentiation approach. Annotations on lines are data types used in the computation.}\label{fig:simplehessian}
\end{figure}

\Fig{fig:simplehessian} shows the data flow in the four passes of computing Hessian. The first two passes obtains the gradients. Before entering the third pass, the program wraps each field in \texttt{GVar} with another layer of \texttt{GVar}. Then we pick a variable $x_i$ and add $1$ to the gradient field of its gradient \texttt{grad(grad($x_i$))} in order to compute the $i$-th row of Hessian. Before entering the final pass, the \texttt{$\sim$GVar} is called. We can not unwrap \texttt{GVar} directly because although the values of gradients have been uncomputed to zero, the gradient fields of gradients may be nonzero. Instead, we use \texttt{Partial\{:x\}(obj)} to take field \texttt{x} of an object without erasing memory. By repeating the above procedure for different $x_i$, one can obtains the full Hessian matrix.

\subsubsection{Hessian propagation}\label{sec:taylor}
A probably more efficient approach is back-propagating Hessians directly~\cite{Martens2012} using the relation
\begin{align}
    \begin{split}
        &H^{\out}_{\vx^{L'},\vx^{L''}} = \mathbf{0},\\
        &H^{\out}_{\vx^{i-1},\vx^{i-1'}} = J^{\vx^i}_{\vx^{i-1}} H^{\out}_{\vx^i, \vx^{i'}} J^{\vx^{i'}}_{\vx^{i-1'}} + J^{\out}_{\vx^i} H^{\vx^i}_{\vx^{i-1}, \vx^{i-1'}}.
    \end{split}
\end{align}
Here, the Hessian tensor $H^{\out}_{\vx^{i-1},\vx^{i-1'}}$ is rank three, where the top index is often taken as a scalar and omitted.
In tensor network diagram, the above equation can be represented as the right panel of \Fig{fig:ad}.
Hessian propagation is a special case of Taylor propagation.
With respect to the order of gradients, Taylor propagation is exponentially more efficient in obtaining higher-order gradients than differentiating lower order gradients recursively. %The later requires traversing the computational graph repeatedly.
%In JAX~\cite{Bettencourt2019}, in order to support Taylor propagation, the propagation rules for part of primitives should be manually defined.
Comparing with operator overloading, source to source automatic differentiation has the advantage of having very limited primitives, exhausted implementation of Hessian propagation is possible.
An example to obtain Hessians is provided in \Sec{sec:bessel}.

\subsection{Gradient on ancilla problem}
In this section, we introduced an easily overlooked problem in our reversible AD framework.
An ancilla sometimes can carry a nonzero gradient when it is going to be deallocated. As a result, even if an ancilla can be uncomputed rigorously in the original program, its \texttt{GVar} wrapped version is not nessesarily safely deallocated.
In NiLang, we simply ``drop'' the gradient field of ancillas instead of raising an error.
In the following, we justify our approach by proving the following theorem
\begin{theorem}
    Deallocating an ancilla with emptied value field and nonzero gradient field does not harm the reversibility of a function.
\end{theorem}
\begin{proof}
    Consider a reversible function $\vx^i, b = f_i(\vx^{i-1}, a)$, where $a$ and $b$ are the input and output values of an ancilla. Since the ancilla is emptied for any input $\vx^{i-1}$, we have
\begin{align}
    \frac{\partial b}{\partial \vx^{i-1}} = \mathbf{0}.
\end{align}
%Suppose in the backward pass, we discard the gradient field of $b$.
%So the question becomes does \texttt{grad(b)} have effect on the $\vx$?
Since the gradient fields are derived from the value fields of variables, discarding gradients should not have any effect to the \texttt{value} fields.
    The rest is to show $\texttt{grad(b)} \equiv \frac{\partial \vx^L}{\partial{b}}$ does appear in the backward rule of this function. It can be seen from the back-propagation rule 
\begin{equation}
    \frac{\partial \vx^L}{\partial \vx^{i-1}} = \frac{\partial \vx^L}{\partial \vx^i}\frac{\partial \vx^i}{\partial \vx^{i-1}} + \frac{\partial \vx^L}{\partial b}\frac{\partial b}{\partial \vx^{i-1}},
\end{equation}
where the second term with $\frac{\partial \vx^L}{\partial{b}}$ vanishes naturally.
\end{proof}

\section{Examples}\label{sec:example}

\subsection{Computing Fibonacci Numbers}\label{sec:fib}
The following is an example that everyone likes, computing Fibonacci number recursively.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
using NiLang

@i function rfib(out!, n::T) where T
    n1 ← zero(T)
    n2 ← zero(T)
    @routine begin
        n1 += identity(n)
        n1 -= identity(1)
        n2 += identity(n)
        n2 -= identity(2)
    end
    if (value(n) <= 2, ~)
        out! += identity(1)
    else
        rfib(out!, n1)
        rfib(out!, n2)
    end
    ~@routine
end
\end{lstlisting}
\end{minipage}

The time complexity of this recursive algorithm is exponential to input \texttt{n}. It is also possible to write a reversible linear time for loop algorithm.
A slightly non-trivial task is computing the first Fibonacci number that greater or equal to a certain number $z$, where a \texttt{while} statement is required.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
@i function rfibn(n!, z)
    @safe @assert n! == 0
    out ← 0
    rfib(out, n!)
    while (out < z, n! != 0)
        ~rfib(out, n!)
        n! += identity(1)
        rfib(out, n!)
    end
    ~rfib(out, n!)
end
\end{lstlisting}
\end{minipage}

In this example, the postcondition \texttt{n!=0} in the \texttt{while} statement is false before entering the loop, and becomes true in later iterations. In the reverse program, the \texttt{while} statement stops at \texttt{n==0}.
If executed correctly, a user will see the following result.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> rfib(0, 10)
(55, 10)

julia> rfibn(0, 100)
(12, 100)

julia> (~rfibn)(rfibn(0, 100)...)
(0, 100)
\end{lstlisting}
\end{minipage}

This examplifies how an addition postcondition provided by user can help reversing a control flow without caching controls.

\subsection{Bessel function}\label{sec:bessel}
An Bessel function of the first kind of order $\nu$ can be computed using Taylor expansion
\begin{equation}
    J_\nu(z) = \sum\limits_{n=0}^{\infty} \frac{(z/2)^\nu}{\Gamma(k+1)\Gamma(k+\nu+1)} (-z^2/4)^{n}
\end{equation}
where $\Gamma(n) = (n-1)!$ is the Gamma function. One can compute the accumulated item iteratively as $s_n = -\frac{z^2}{4} s_{n-1}$. The irreversible implementation is

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
function besselj(ν, z; atol=1e-8)
    k = 0
    s = (z/2)^ν / factorial(ν)
    out = s
    while abs(s) > atol
        k += 1
        s *= (-1) / k / (k+ν) * (z/2)^2
        out += s
    end
    out
end
\end{lstlisting}
\end{minipage}

This computational process could be diagramatically represented as a DAG as show in \Fig{fig:computational-graph}.
The problem of this diagram is the data is represented as an edge, while in a simple graph, an edge connects at most two nodes. This underlines the fact that in a traditional computational graph, a variable can only be used in two functions, one as output and one as input. This also explains why DAG based computational graph can not be used to describe inplace functions, where data are used by multiple operations.
For example, the red lines in the graph used to represent the reused variables in a \texttt{while} statement is in fact forbidden, it creates circles in the graph.
\begin{figure}
    \centerline{\includegraphics[width=0.6\columnwidth,trim={0cm 0cm 0 0},clip]{images/computational_graph.pdf}}
    \caption{Traditional computational graph for the reversible first kind Bessel function. Where a vertex (circle) is an operation and a directed edge is a variable. The gray region is the body of while loop.}\label{fig:computational-graph}
\end{figure}

In the following, we introduce the reversible implementation and the data oriented computational graph.
Intuitively, this problem mimics the famous pebble game~\cite{Perumalla2013}, since one can not release state $s_{n-1}$ directly after computing $s_n$.
One would need an increasing size of tape to cache the intermediate state.
To circumvent this problem. We introduce the following reversible approximate multiplier

\begin{minipage}{.44\textwidth}
    \begin{lstlisting}[numberstyle=\scriptsize\color{gray},numbers=left,numbersep=8pt]
@i @inline function imul(out!, x, anc!)
    anc! += out! * x
    out! -= anc! / x
    SWAP(out!, anc!)
end
\end{lstlisting}
\end{minipage}

Here, the definition of SWAP can be found in \App{app:instr}, $\texttt{anc!} \approx 0$ is a \textit{dirty ancilla}.
Line 2 computes the result and accumulates it to the dirty ancilla, we get an approximately correct output in \texttt{anc!}.
Line 3``uncomputes'' \texttt{out!} approximately by using the information stored in \texttt{anc!}, leaving a dirty zero state in register \texttt{out!}.
Line 4 swaps the contents in \texttt{out!} and \texttt{anc!}.
Finally, we have an approximately correct output and a dirtier ancilla.
With this multiplier, we implementation $J_\nu$ as follows.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
using NiLang, NiLang.AD

@i function ibesselj(out!, ν, z; atol=1e-8)
    k ← 0
    fact_nu ← zero(ν)
    halfz ← zero(z)
    halfz_power_nu ← zero(z)
    halfz_power_2 ← zero(z)
    out_anc ← zero(z)
    anc1 ← zero(z)
    anc2 ← zero(z)
    anc3 ← zero(z)
    anc4 ← zero(z)
    anc5 ← zero(z)

    @routine begin
        halfz += z / 2
        halfz_power_nu += halfz ^ ν
        halfz_power_2 += halfz ^ 2
        ifactorial(fact_nu, ν)

        anc1 += halfz_power_nu/fact_nu
        out_anc += identity(anc1)
        while (abs(unwrap(anc1)) > atol &&
                abs(unwrap(anc4)) < atol, k!=0)
            k += identity(1)
            @routine begin
                anc5 += identity(k)
                anc5 += identity(ν)
                anc2 -= k * anc5
                anc3 += halfz_power_2 / anc2
            end
            imul(anc1, anc3, anc4)
            out_anc += identity(anc1)
            ~@routine
        end
    end
    out! += identity(out_anc)
    ~@routine
end

@i function ifactorial(out!, n)
    out! += identity(1)
    for i=1:n
        MULINT(out!, i)
    end
end
\end{lstlisting}
\end{minipage}

Here, only a constant number of ancillas are used in this implementation, while the algorithm complexity does not increase comparing to its irreversible counterpart.
ancilla \texttt{anc4} plays the role of \textit{dirty ancilla} in multiplication, it is uncomputed rigoriously in the uncomputing stage.
The reason why the ``approximate uncomputing'' trick works here lies in the fact that from the mathematic perspective the state in $n$th step $\{s_n, z\}$ contains the same amount of information as the state in the $n-1$th step $\{s_{n-1}, z\}$ except some special points, it is highly possible to find an equation to uncompute the previous state from the current state.
This trick can be used extensively in many other application. It mitigated the artifitial irreversibility brought by the number system that we have adopt at the cost of precision.
%The \texttt{while} statement takes two conditions, the precondition and postcondition. Precondition \texttt{val(anc1) > atol} indicates when to break the forward pass and postcondition \texttt{iplus != 0} indicates when to break the backward pass.

This reversible program can be diagramatically represented as a directed hypergraph as show in \Fig{fig:datagraph}.
The key difference is we adopt a data centric view.
In this view, we classify the arguments of an operation into control parameters and mutable parameters.
A control parameter is represented as a dot, it does not change after the operation.
We can using this property to analyse uncomputable variables. For example in routine ``B'' \texttt{anc2}, \texttt{anc3} and \texttt{anc5} are changed in the compute session.
\texttt{anc2} and \texttt{anc4} are changed in the copy session, while \texttt{anc3} is used but not changed in function \texttt{imul}.
Hence we can call the inverse routine \texttt{$\sim$B} to safely uncompute \texttt{anc2}, \texttt{anc3} and \texttt{anc5} and reuse them in the next iteration.
It is also clear that the values of \texttt{z} and \texttt{$\nu$} are not changed in this function, while \texttt{out!} is changed to the desired output. It can be represented as the following.
\begin{figure}[htp]
    \includegraphics[width=0.4\columnwidth,trim={0cm 1.5cm 0cm 1.0cm},clip]{images/fig5.pdf}
\end{figure}

\begin{figure}
    \centerline{\includegraphics[width=\columnwidth,trim={1cm 1cm 0 0},clip]{images/fig4.pdf}}
    \caption{Data oriented computational graph for the reversible first kind Bessel function. Circles connected by vertical lines are variables, while boxes and dots connected by parallel lines are operations. A dot on a line represents a control parameter while a box represents a mutable parameter. Annotations $\#1, \#2, \ldots$ are used the mark the positions of variables in the argument list. The gray box represents the body of a while loop, where a precondition and postcondition are positioned on top and bottom of this box.}\label{fig:datagraph}
\end{figure}

To obtain gradients, one can wrap the variable \texttt{y!} with \texttt{Loss} type and feed it into \texttt{ibesselj\textquotesingle}

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> y, x = 0.0, 3.0
(0.0, 3.0)

julia> ibesselj'(Loss(y), 2, x)
(Loss(GVar(0.0, 1.0)), 2, GVar(3.0, 0.0149981304))
\end{lstlisting}
\end{minipage}

Here, \texttt{ibesselj\textquotesingle} is a callable instance of type \texttt{Grad\{typeof(ibesselj)\}}. Its implementation is shown in \Sec{sec:jacobian}. This function itself is reversible and differentiable, one can back-propagate this function to obtain Hessians as introduced in \Sec{sec:simplehessian}. In NiLang, it is implemented as \texttt{hessian\_repeat}.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> hessian_repeat(ibesselj, (Loss(y), 2, x))
3×3 Array{Float64,2}:
 0.0  0.0   0.0    
 0.0  0.0   0.0    
 0.0  0.0  -0.27505
\end{lstlisting}
\end{minipage}

%The loss variable is specified by a wrapper \texttt{Loss}, notice we don't distinguish input and output in reversible programming.
%The gradient function is implemented reversibally so that the gradient field of output can be differentiated again to obtain Hessians as shown in \Fig{fig:simplehessian}.

To obtain Hessians, we can also use the Hessian propagation approach as introduced in \Sec{sec:taylor}.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> ibesselj''(Loss(y), 2, x)
(Loss(BeijingRing{Float64}(0.0, 1.0, 1)), 2, 
BeijingRing{Float64}(3.0, 0.014998134978750133, 2))

julia> collect_hessian()
2×2 Array{Float64,2}:
 0.0   0.0    
 0.0  -0.27505
\end{lstlisting}
\end{minipage}

\texttt{ibesselj\textquotesingle\textquotesingle} computes the second-order gradients. It wraps variables with type \texttt{BeijingRing}~\footnote{When people ask for the location in Beijing, they will start by asking which ring it is? We use the similar approach to locate the elements of Hessian matrix.} in the backward pass. \texttt{BeijingRing} records Jacobians and Hessians for a variable, where Hessians are stored in a global storage.
Whenever an $n$-th variable or ancilla is created, we push a ring of size $2n-1$ to a global tape. Whenever an ancilla is deallocated, we pop a ring from the top. The $n$-th ring stores Hessian elements $H_{i\leq n,n}$ and $H_{n,i<n}$.% We didn't use the symmetry relation $H_{i,j} = H_{j,i}$ to save memory here in order to simplify the implementation of backward rules described in the right most panel of \Fig{fig:ad}.
The final result can be collected by calling \texttt{collect\_hessian()}, which will read out the Hessian matrix that stored in the global storage.

\subsection{Unitary Matrices}\label{sec:umm}
A unitary matrices features uniform eigenvalues and reversibility. It is widely used as an approach to ease the gradient exploding and vanishing problem~\cite{Arjovsky2015,Wisdom2016,Li2016} and the memory wall problem~\cite{Luo2019}.
One of the simplest way to parametrize a unitary matrix is representing a unitary matrix as a product of two-level unitary operations~\cite{Li2016}. A real unitary matrix of size $N$ can be parametrized compactly by $N(N-1)/2$ rotation operations~\cite{Li2013}
\begin{align}
    {\rm ROT}(a!, b!, \theta)  = \left(\begin{matrix}
        \cos(\theta) & - \sin(\theta)\\
        \sin(\theta)  & \cos(\theta)
    \end{matrix}\right)
    \left(\begin{matrix}
        a!\\
        b!
    \end{matrix}\right),
\end{align}
where \texttt{$\theta$} is the rotation angle, \texttt{a!} and \texttt{b!} are target registers.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}[mathescape=true]
using NiLang, NiLang.AD

@i function umm!(x!, θ)
    @safe @assert length(θ) == 
            length(x!)*(length(x!)-1)/2
    k ← 0
    for j=1:length(x!)
        for i=length(x!)-1:-1:j
            k += identity(1)
            ROT(x![i], x![i+1], θ[k])
        end
    end

    k → length(θ)
end
\end{lstlisting}
\end{minipage}

Here, the ancilla \texttt{k} is deallocated manually by specifying its value, because we know the loop size is $N(N-1)/2$.
We define the test functions in order to check gradients.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}[mathescape=true]
julia> @i function isum(out!, x::AbstractArray)
           for i=1:length(x)
               out! += identity(x[i])
           end
       end

julia> @i function test!(out!, x!::Vector, θ::Vector)
           umm!(x!, θ)
           isum(out!, x!)
       end

julia> out, x, θ = Loss(0.0), randn(4), randn(6);

julia> @instr test!'(out, x, θ)

julia> x
4-element Array{GVar{Float64,Float64},1}:
 GVar(1.220182125326287, 0.14540743042341095) 
 GVar(2.1288634811475937, -1.3749962375499805)
 GVar(1.2696579252569677, 1.42868739498625)   
 GVar(0.1083891125379283, 0.2170123344615735) 

julia> @instr (~test!')(out, x, θ)

julia> x
4-element Array{Float64,1}:
 1.220182125326287  
 2.1288634811475933 
 1.2696579252569677 
 0.10838911253792821
\end{lstlisting}
\end{minipage}

In the above testing code, \texttt{test\textquotesingle} attaches a gradient field to each element of \texttt{x}. \texttt{$\sim$test\textquotesingle} is the inverse program that erase the gradient fields.
Notablly, this reversible implementation costs zero memory allocation although it changes the target variables inplace.

%It implements the backward rule of a \texttt{ROT} instruction
%\begin{align}
%    \begin{split}
%    \overline{\theta}  &= \sum\frac{\partial R(\theta)}{\partial \theta}\odot(\overline{y}x^T)\\
%    &= \Tr\left[\frac{\partial R(\theta)}{\partial \theta}^T\overline{y}x^T\right]\\
%    &= \Tr\left[R\left(\frac{\pi}{2}-\theta\right)\overline{y}x^T\right]
%    \end{split}
%\end{align}

\subsection{QR decomposition}

Let's consider a naive implementation of QR decomposition from scratch.
We admit this implementation is just a proof of principle which does not even consider reorthogonalization.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
using NiLang, NiLang.AD

@i function qr(Q, R, A::Matrix{T}) where T
    anc_norm ← zero(T)
    anc_dot ← zeros(T, size(A,2))
    ri ← zeros(T, size(A,1))
    for col = 1:size(A, 1)
        ri .+= identity.(A[:,col])
        for precol = 1:col-1
            dot(anc_dot[precol], Q[:,precol], ri)
            R[precol,col] += 
                identity(anc_dot[precol])
            for row = 1:size(Q,1)
                ri[row] -= 
                    anc_dot[precol] * Q[row, precol]
            end
        end
        norm2(anc_norm, ri)

        R[col, col] += anc_norm^0.5
        for row = 1:size(Q,1)
            Q[row,col] += ri[row] / R[col, col]
        end

        ~begin
            ri .+= identity.(A[:,col])
            for precol = 1:col-1
                dot(anc_dot[precol], Q[:,precol], ri)
                for row = 1:size(Q,1)
                    ri[row] -= anc_dot[precol] *
                        Q[row, precol]
                end
            end
            norm2(anc_norm, ri)
        end
    end
end
\end{lstlisting}
\end{minipage}

Here, in order to avoid frequent uncomputing, we allocate ancillas \texttt{ri} and \texttt{anc\_dot} as vectors.
The expression in $\sim$ is used to uncompute \texttt{ri}, \texttt{anc\_dot} and \texttt{anc\_norm}.
%\texttt{R[col, col] += anc\_norm${}^\wedge$0.5} is a ternary instruction, whose backward rule is defined in NiLang.
\texttt{dot} and \texttt{norm2} are reversible functions to compute dot product and vector norm.
Their implementations could be found in \App{app:functions}.
One can easily check the correctness of the gradient function

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> A, q, r = randn(4,4), zero(A), zero(A);

julia> @i function test1(out, q, r, A)
           qr(q, r, A)
           isum(out, q)
       end

julia> check_grad(test1, (Loss(0.0), q, r, A))
true
\end{lstlisting}
\end{minipage}

Here, the loss function \texttt{test1} is defined as the sum of the output unitary matrix \texttt{q}. The \texttt{check\_grad} function is a gradient checker function defined in module NiLang.AD.
%It should be possible to define other linear algebra functions reversiblly too. We leave this future projects.

\subsection{Solving a graph embedding problem}\label{sec:graph}

Consider the Petersen graph as shown in \Fig{fig:petersen}.
\begin{figure}
    \centerline{\includegraphics[width=0.6\columnwidth,trim={0 1cm 0 0},clip]{images/petersen.pdf}}
    \caption{The Petersen graph with 10 vertices and 15 edges.}\label{fig:petersen}
\end{figure}


We want to embed it to a $k$ dimensional Euclidean space, so that the distance between a pair of neighboring vertices is $l_1$ while the distance between a pair o disconnected vertices is $l_2$. Find the minimum possible $k$.
This problem, can be solved by differential pogramming by designing the following loss.
\begin{align}
    \begin{split}
        \mathcal{L} &= \sigma^2(\{|\vec r_i - \vec r_j|^2 \mid  i,j ~\text{are connected}\}) \\
        &+ \sigma^2(\{|\vec r_i - \vec r_j|^2 \mid i,j ~\text{are not connected}\}\}) + 
    \end{split}
\end{align}
where $\vec r_i \in \mathbb{R}^k$ is the position of $i$th vertex.
We seperate vertex pairs into two categories by connectivity and compute the variance of squared distance in each category.
The loss is defined as the summation of variances.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
using NiLang, NiLang.AD

@i function sqdistance(dist!, x1::AbstractVector{T},
        x2::AbstractVector) where T
    @inbounds for i=1:length(x1)
        x1[i] -= identity(x2[i])
        dist! += x1[i] ^ 2
        x1[i] += identity(x2[i])
    end
end

@i function iloss(out!::T, x) where T
    v1 ← zero(T)
    m1 ← zero(T)
    v2 ← zero(T)
    m2 ← zero(T)
    diff ← zero(T)
    L1 ← [(1, 6), (2, 7), (3, 8), (4, 9), (5, 10),
    (1, 2), (2, 3), (3, 4), (4, 5), (1, 5), (6, 8),
    (8, 10), (7, 10), (7, 9), (6, 9)]
    L2 ← [(1, 3), (1, 4), (1, 7), (1, 8), (1, 9),
    (1, 10), (2, 4), (2, 5), (2, 6), (2, 8), (2, 9),
    (2, 10), (3, 5), (3, 6), (3, 7), (3, 9), (3, 10),
    (4, 6), (4, 7), (4, 8), (4, 10), (5, 6), (5, 7),
    (5, 8), (5, 9), (6, 7), (6, 10), (7, 8), (8, 9),
    (9, 10)]
    d1 ← zeros(T, length(L1))
    d2 ← zeros(T, length(L2))
    @routine begin
        for i=1:length(L1)
            sqdistance(d1[i], x[:,L1[i][1]],
                x[:,L1[i][2]])
        end
        for i=1:length(L2)
            sqdistance(d2[i], x[:,L2[i][1]],
                x[:,L2[i][2]])
        end
        var_and_mean(v1, m1, d1)
        var_and_mean(v2, m2, d2)
    end
    out! += identity(v1)
    out! += identity(v2)
    ~@routine
end
\end{lstlisting}
\end{minipage}

One can access the gradient by typing

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
iloss'(Loss(0.0), randn(5, 10))
\end{lstlisting}
\end{minipage}

In the training, we fixed two of the vertices and train the rest, otherwise the program will find the trivil solution of all same location. 
The training uses Adam optimizer~\cite{}, a $20000$ steps training costs less than 2 seconds on a laptop. For $k=4$, we can get a loss close to machine precision, while for $k < 4$, the loss is much higher than $0$.
From the solution, it is easy to verify $l_2/l_1 = 0$.

\section{Discussion and outlook}\label{sec:discussion}
In this paper, we show a program on an reversible Turing machine can be differentiated to any order reliably and efficiently without sophisticated designs to memorize computational graph and intermediate states. 
We introduce a reversible Julia eDSL NiLang that implements a reversible AD. In a reversible programming language, we proposed to use ``approximate uncomputing'' trick to avoid the overhead of a reversible program in many practical cases.

In the following, we discussed some practical issues about reversible programming, and several future directions to go.
%Notablely, we introduce the concept of ``arithematic uncomputing'' to reduce the overhead of recursive reversible algorithms.

\subsection{Time Space Tradeoff}\label{sec:timespace}
In history, there has been many other interesting designs of reversible languages. However, current popular programming languages are all irreversible.
%One of the main reason why RTM is not so popular is it has 
In the simplest g-segment trade off scheme~\cite{Bennett1989,Levine1990}, a RTM model has either a space overhead that proportional to computing time $T$ or a computational overhead that sometimes can be exponential comparing with a irreversible counter part.
The tradeoff between space and time is a crutial issue in the theory of RTM.
In the following, we try to convince the readers that the overhead of reversible computing is not as terrible as people thought.
%we have
%\begin{align}
%    Time(T) &= \frac{T^{1+\epsilon}}{S^\epsilon},\\
%    Space(T) &= \epsilon 2^{1/\epsilon}(S+S\log\frac{T}{S}).
%\end{align}
%Here, $T$ and $S$ are the time and space usage on a irreversible Turing machine. $\epsilon$ is the control parameter.
%It is related to the g-segment trade off parameters by $g = k^n, \epsilon = \log_k(2k-1)$ with $n\geq 1$ and $k\geq 1$.
%First, let $\epsilon \rightarrow 0$, there is not overhead in time. 

The overhead of reversing a program is bounded the checkpointing~\cite{Chen2016} strategy used in a traditional machine learning package that memorizes every inputs of primitives because similar strategy can also be used in reversible programming.~\cite{Perumalla2013}
% Memorizing the inputs always make a primitive reversible since it does not discard any information.
%For deep neural networks, people used checkpointing trick to trade time with space~\cite{Chen2016}. This trick is also widely used in reversible programming~\cite{Perumalla2013}. 
Reversible programming simply provides more alternatives to reduce the overhead.
%For inplace functions, especially those reversible functions. Reversible programming AD is sometimes more memory efficient. Comparing with logging computational graph.
%Second, many computational overheads come from of the irreversibility of \texttt{/=} and \texttt{*=} operations. This part is not fundamental because reversible floating point instructions have already been designed~\cite{Nachtigal2010,Nachtigal2011}. Using reversible floating point instructions may significant decrease the computation time and memory usage of a RTM.
%Even in current stage, 
For example, the overhead in many iterative algorithms can often be removed with ``arithematic uncomputing'' trick without sacrificing reversibility as shown in the \texttt{ibesselj} example in \Sec{sec:bessel}.
% We will review this point in \Sec{sec:hardware}.

Clever compiling can also be used to remove most overheads.
Often, when we define a new reversible function, we allocate some ancillas at the beginning of the function and deallocate them through uncomputing at the end.
The overhead comes from the uncomputing, in the worst case, the time used for uncomputing can be the same as the forward pass.
In a hierarchical design, uncomputing can appear in every layer of abstraction. To quantify the overhead of uncomputing, we introducing the concept
\begin{definition}[program granularity]
    The log ratio between the execution time of a reversible program and its irreversible counter part
    %\begin{equation}
        %\log_2 \frac{Time(T)}{T}.
    %\end{equation}
\end{definition}
The computing time increases exponentially as the granularity increases.
A cleverer compilation of a program can reduce the granularity by merging the uncomputing statements to avoid repeated efforts.

At last, making reversible programming an eDSL rather than an independent language allows flexible choices between reversibility and computational overhead. For example, in order to deallocate the gradient memory in a reversible language one has to uncompute the whole process of obtaining this gradient.
In our eDSL, we can just deallocate the memory irreversibly, i.e. trade energy with time.


%One should notice the memory advantage of reversible programming to machine learning does comes from reversibility itself, but from a better data tracking strategy inspired from invertible programming.
%Normally, a reversible program is not as memory efficient as its irreversible couterpart due to the additional requirement of no information loss. A naive approach that keeping track of all information will cost an additional space $O(T)$, where $T$ stands for the excution time in a irreversible TM, the longer the program runs, the larger the memory usage is. This is exactly the approach to keeping reversibility in most machine learning packages in the market.
%The point it, an reversible Turing Machine is able to trade space with time.
%In some cases, it may cause polynomial overhead than its irreversible counterpart.

\subsection{Instructions and Hardwares}\label{sec:hardware}
%Todays CPU are starving, that is, the memory access is the performance bottleneck in many applications rather than the arithmetic operations.
%There is a natural granularity for operations with memory access or not.
So far, our eDSL is compiled to Julia.
In the future, it can be compiled to reversible instructions~\cite{Vieri1999} and executed on a reversible device.
%For example, the control flow defined in this NiLang can be compiled to reversible instructions like reversible \texttt{goto} instruction, where
%the target instruction can be a \texttt{comefrom} instruction that specifing the postcondition.
However, arithmetic instructions should be redesigned to support better reversible programs.
The major obstacle to exact reversibility programming is current floating point adders and multipliers used in our computing devices are not exactly reversible.
There are proposals of reversible floating point adders and multipliers~\cite{Nachtigal2010,Nachtigal2011,Nguyen2013,Hner2018}, however these designs with allocate garbage bits in each operaton.
%, which is not too different from the information buffer approach~\cite{Maclaurin2015}.
%In other words, to represent a 64 bit floating point number requires more than 64 bits in storage. Reversible multiplier is also possible in similar approach.~\cite{Nachtigal2010} 
%With floating point numbers, rigorous reversible arithematic designs without using information buffer or garbage qubits is nearly impossible.
Alternatives include fixed point numbers~\cite{FixedPointNumbers} and logarithmic numbers~\cite{Taylor1988,LogarithmicNumbers}, where logarithmic number system is reversible under \texttt{*=} and \texttt{/=}.
With these infrastructures, a reversible program can be executed without suffering from the rounding error.

%Reversible programming is not nessesarily related to reversible hardwares.
%Reversible programs is a subset of irreversible programs, hence can be simulated efficiently on traditional CMOS devices~\cite{Vieri1999}.
%Reversible programming just provides an alternative to execute on an energy efficient reversible hardwares.
Reversible instructions can be executed on an energy efficient reversible hardware.
In the introduction, we mensioned several reversible hardwares. A reversible hardware can be those supporting reversible gates such as the Toffoli gate and the Fredkin gate, or like an adiabatic CMOS with the ability to recover signal energy. The latter is known as the generalized reversible computing.~\cite{Frank2005,Frank2017b}
In the near future, there might be energy efficient artificial intellegence (AI) chips as coprocessors that our eDSL can compile to.
Since reversible computing is mainly driven by quantum computing in recent years.
In the following, we comment briefly on quntum devices.

\subsubsection{Quantum Computers}\label{sec:qc}
Building a universal quantum computer~\cite{Nielsen2002} is difficult. 
The difficulty lies in the fact that it is extremely hard to protect a quantum state.
Unlike a classical state, an quantum state is can not be cloned, meanwhile, it losses information by interating with the environment, or decoherence.
%These two facts underlines the simulation nature of quantum devices.
%Although there are proposals about quantum random access memory~\cite{Giovannetti2008}, they are difficult to implement, and are known to have many caveats~\cite{Aaronson2015}.
%In the era of noisy intermediate sized quantum devices, more and more people are switching to classical-quantum hybrid devices, where a quantum device plays the role of a programmable simulator.
Classical reversible computing does not enjoy the quantum advantage, nor the quantum disadvantages of non-cloning and decoherence.
%The reversibility of quantum computing comes from the fact that microscopic processes are unitary.
%On the other side, the irreversibility is rare, it can come from interacting with classical devices. Irreversible processes include decaying, qubit state resetting, measurements and classical feed backs to quantum devices. These are typically harder to implement on a quantum device.
%Given the fundamental limitations of quantum decoherence and non-cloning and the reversible nature of microscopic world.
It is technically more smooth to have a reversible computing device to bridge the gap between classical devices and universal quantum computing devices.
By introducing entanglement little by little, we can accelerate some basic components in reversible computing. For example, quantum Fourier transformation provides an interesting alternative to the reversible adders and multipliers by introducing the CPHASE quantum gate.~\cite{RuizPerez2017}
The development of reversible compiling theory can be benefit quantum compiling directly.

\subsection{Outlook}\label{sec:outlook}
The reversible eDSL NiLang can be used to solve many existing scientific computing problems.
First of all, it can be used to generate AD rules for existing machine learning packages like Zygote.
For example, one can use NiLang to generate backward rules for singular value decoposition and eigenvalue decomposition functions that extensively used in scientific computing~\cite{Xie2020,Liao2019}.
Although their backward rules~\cite{Seeger2017,Wan2019,Hubig2019} have been drived in recent years, these backward rules can not handle degenerate eigenvalues properly. Hopefully, the automatically generated backward rules do not have such problems.

Secondly, we can use it to overcome the memory wall problem in some applications.
NiLang provides a systematic time-space trade off scheme through uncomputing.
A successful related example is the memory efficient domain-specific AD engine in quantum simulator Yao~\cite{Luo2019}.
This domain-specific AD engine is written in a reversible style and solved the memory bottleneck in variational quantum simulations. It also gives hitherto the best performance in differentiating quantum circuit parameters.
Similarily, we can write memory efficient normalizing flow~\cite{Kobyzev2019} with NiLang. Normalizing flow is a successful class of generative model in both computer vision~\cite{Kingma2018} and quantum physics~\cite{Dinh2016,Li2018}, where its building block bijector is reversible.
We can use similar idea to differentiate reversible integrators~\cite{Hut1995, Laikov2018}.
With reversible integrators, it should be possible to rewrite the control system in robotics~\cite{Giftthaler2017} in a reversible style, where scalars are first class citizen rather than tensors.
Writing a control program reversibly should boost the training performance a lot.

Thirdly, reversibility is a resource for training.
For those who are interested in non-gradient based training. In \App{sec:train}, we provide a self-consistency training strategy for reversible programs.

Latstly, the reversible IR is a good starting point to study quantum compiling. Most quantum programming language preassumes a classical coprocessor and use classical control flows~\cite{Svore2018} in universal quantum computing.
However, we believe reversible control flows are also very important to a universal quantum computer.

To solve the above problems better, NiLang can be improved from multiple perspectives.
Like we need a more efficient fixed point or log number system to avoid rounding errors. Currently we simulate reversible arithematics with the julia fixed point numebr package.~\cite{FixedPointNumbers}
Then we should optimize the compiling to decreases granularity of a program and reduces uncomputing overheads.
There are also some known issues to be solved like the type inference problem, we have listed some of them on Github.
%NiLang also need better type inference support. Current type inference assumes the variable types not changed by a function, which is not true.
%The type cast rule of a reversible function should be recorded somewhere to help type inference.
These improvements needs participation of people from multiple fields.

% By porting a quantum simulator. 
%it is interesting to see how quantum simulator can improve the instruction design. Notice a quantum fourier transformation (QFT) based quantum adder and multiplier is sometimes more efficient than a classical adder~\cite{Haener2018} \blue{Is this true?}.
%This could be used to reduce the momory cost in normalizing flow, time-reversible integrator, recurrent neural network and residual neural network.

\section{acknowledgments}
Jin-Guo Liu thank Lei Wang for motivating the project with possible applications reversible integrator, normalizing flow and neural ODE.
Xiu-Zhe Luo for discussion on the implementation details of source to source automatic differetiation,
Shuo-Hui Li for helpful discussion on differential geometry.
Damian Steiger for telling me the \texttt{comefrom} joke.
Tong Liu and An-Qi Chen for helpful discussion on quantum adders and multipliers.
Chris Rackauckas for helpful discussion on reversible integrators.
Mike Innes for reviewing the comments about Zygote.
Jun Takahashi for providing the facinating graph embedding problem.
Simon Byrne and Chen Zhao for helpful discussion on floating point and logarithmic numbers.
The authors are supported by the National Natural Science Foundation of China under the Grant No.~11774398, the Strategic Priority Research Program of Chinese Academy of Sciences Grant No.~XDB28000000.

\bibliographystyle{apsrev4-1}
\bibliography{invc}

\pagebreak
\appendix

\section{NiLang Grammar}\label{app:grammar}

To define a reversible function one can use ``@i'' plus a normal function definition like bellow

\begin{minipage}{.44\textwidth}
\begin{lstlisting}[basicstyle=\small\ttfamily,columns=fullflexible]
"""
docstring...
"""
@i function f(args..., kwargs...) where {...}
    <stmts>
end
\end{lstlisting}
\end{minipage}
where the definition of ``<stmts>'' are shown in the grammar on the next column.
The following is a list of terminologies used in the definition of grammar
\begin{itemize}
    \item $ident$, symbols
    \item $num$, numbers
    \item $\epsilon$, empty statement
    \item $JuliaExpr$, native Julia expression
    \item $[$ $]$,  zero or one repetitions.
\end{itemize}
Here, all $JuliaExpr$ should be pure, otherwise the reversibility is not guaranteed.
Dataview is a view of a data, it can be a bijective mapping of an object, an item of an array or a field of an object.

\newpage

\begin{minipage}{0.3\textwidth}
    \small
\input{bnf.tex}
\end{minipage}

\section{Instructions and Backward Rules}\label{app:instr}

The translation of instructions to Julia functions
\begin{table}[h!]\centering
\begin{minipage}{\columnwidth}
\ra{1.3}
    \scalebox{1.0}{
        \begin{tabularx}{\textwidth}{X X c}\toprule
            \textbf{instruction} & \textbf{translated} & \textbf{symbol}\\
            \hline
            $y \pluseq f(args...)$ & \texttt{PlusEq(f)(args...)} & $\oplus$\\
            $y \minuseq f(args...)$ & \texttt{MinusEq(f)(args...)} & $\ominus$\\
            $y \mathrel{\veebar}= f(args...)$ & \texttt{XorEq(f)(args...)} & $\odot$\\
            \bottomrule
        \end{tabularx}
    }
    \caption{Instructions and their compilation in NiLang.}\label{tbl:interp}
\end{minipage}
\end{table}


The list of instructions implemented in NiLang
\begin{table}[h!]\centering
\begin{minipage}{\columnwidth}
\ra{1.3}
    \scalebox{1.0}{
        \begin{tabularx}{\textwidth}{X X}\toprule
            \textbf{instruction} & \textbf{output}\\
            \hline
            ${\rm SWAP}(a, b)$ & $b, a$\\
            ${\rm ROT}(a, b, \theta)$ & $a \cos\theta - b\sin\theta, b \cos\theta + a\sin\theta, \theta$\\
            ${\rm IROT}(a, b, \theta)$ & $a \cos\theta + b\sin\theta, b \cos\theta - a\sin\theta, \theta$\\
            $y \pluseq a^\wedge b$ & $y+a^b, a, b$\\
            $y \pluseq \exp(x)$ & $y+e^x, x$\\
            $y \pluseq \log(x)$ & $y+\log x, x$\\
            $y \pluseq \sin(x)$ & $y+\sin x, x$\\
            $y \pluseq \cos(x)$ & $y+\cos x, x$\\
            $y \pluseq {\rm abs}(x)$ & $y+ |x|, x$\\
            ${\rm NEG}(y)$ & $-y$\\
            ${\rm CONJ}(y)$ & $y'$\\
            \bottomrule
        \end{tabularx}
    }
    \caption{A collection of reversible instructions, ``.'' is the broadcasting operations in Julia.}\label{tbl:revinstructions}
\end{minipage}
\end{table}

\subsection{Backward rules for instructions}\label{app:jacobians}
For function $\vec{y} = f(\vec{x})$, its Jacobian is $J_{ij} = \frac{\partial y_i}{\partial x_j}$ and its Hessian is $H^k_{ij} = \frac{\partial y_k}{x_i x_j}$.
We have the following local Jacobians and Hessians on the above instructions.

\begin{enumerate}
    \item $a \mathrel+= b$

\begin{align*}
    &J = \left(\begin{matrix}
1 & 1\\
0 & 1
\end{matrix}\right)\\
    &H = \mathbf{0}
\end{align*}

The inverse is $a \mathrel-= b$, its Jacobian is the inverse of the matrix above

\begin{align*}
J(f^{-1}) = J^{-1} = \left(\begin{matrix}
1 & -1\\
0 & 1
\end{matrix}\right)
\end{align*}

In the following, we omit the Jacobians and Hessians of inverse functions.

\item $a\mathrel+=b*c$

\begin{align*}
    &J = \left(\begin{matrix}
1 & c & b\\
0 & 1 & 0\\
0 & 0 & 1
\end{matrix}\right)\\
    &H^a_{bc} = H^a_{cb} = 1, else ~0
\end{align*}

\item $a\mathrel+=b/c$

\begin{align*}
    &J = \left(\begin{matrix}
1 & 1/c &-b/c^2\\
0 & 1 & 0\\
0 & 0 & 1
\end{matrix}\right)\\
    &H^a_{cc} = 2b/c^3,\\
    &H^a_{bc} = H^a_{cb} = -1/c^2, else ~ 0
\end{align*}

\item $a\mathrel+=b^c$


\begin{align*}
    &J = \left(\begin{matrix}
1 &  cb^{c-1} &   b^c \log b \\
0 & 1 & 0\\
0 & 0 & 1
\end{matrix}\right)\\
    &H^a_{bc} = H^a_{cb} = b^{c-1} + c b^{c-1}\log b,\\
    &H^a_{bb} = (c-1)c b^{c-2},\\
    &H^a_{cc} = b^c\log^2b, else ~0
\end{align*}

\item $a\mathrel+=e^b$

\begin{align*}
    &J = \left(\begin{matrix}
1 &  e^b \\
0 & 1
\end{matrix}\right)\\
    &H^a_{bb} = e^b, else ~0
\end{align*}

\item $a\mathrel+=\log b$

\begin{align*}
    &J = \left(\begin{matrix}
1 &  1/b \\
0 & 1
\end{matrix}\right)\\
    &H^a_{bb} = -1/b^2, else ~0
\end{align*}

\item $a\mathrel+=\sin b$

\begin{align*}
    &J = \left(\begin{matrix}
1 &  \cos b \\
0 & 1
\end{matrix}\right)\\
    &H^a_{bb} = -\sin b, else ~0
\end{align*}

\item $a\mathrel+=\cos b$

\begin{align*}
    &J = \left(\begin{matrix}
1 &  -\sin b \\
0 & 1
\end{matrix}\right)\\
    &H^a_{bb} = -\cos b, else ~0
\end{align*}

\item $a \mathrel+= \vert b\vert$

\begin{align*}
    &J = \left(\begin{matrix}
1 &  {\rm sign} (b) \\
0 & 1
\end{matrix}\right)\\
    &H = \mathbf{0}
\end{align*}

\item $a = -a$

\begin{align*}
    &J = \left(\begin{matrix}
-1
\end{matrix}\right)\\
    &H = \mathbf{0}
\end{align*}

\item ${\rm SWAP}(a, b) = (b, a)$

\begin{align*}
    &J = \left(\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right)\\
    &H = \mathbf{0}
\end{align*}

\item \begin{align*}
{\rm ROT}(a, b, \theta)  = \left(\begin{matrix}
        \cos\theta & - \sin\theta\\
        \sin\theta  & \cos\theta
    \end{matrix}\right)
    \left(\begin{matrix}
        a\\
        b
    \end{matrix}\right)
\end{align*}

\begin{align*}
    &J = \left(\begin{matrix}
        \cos\theta & - \sin\theta & -b\cos\theta-a\sin \theta\\
        \sin\theta  & \cos\theta & a\cos\theta -b\sin\theta\\
        0 & 0 & 1
    \end{matrix}\right)\\
    &H^a_{a\theta} = H^a_{\theta, a} = -\sin\theta,\\
    &H^a_{b\theta} = H^a_{\theta, b} = -\cos\theta,\\
    &H^a_{\theta\theta} = -a\cos\theta + b\sin\theta,\\
    &H^b_{a\theta} = H^b_{\theta a} = \cos\theta,\\
    &H^b_{b\theta} = H^b_{\theta b} = -\sin\theta,\\
    &H^b_{\theta\theta} = -b\cos\theta-a\sin\theta, else ~0
\end{align*}
\end{enumerate}

\section{Learn by consistency}\label{sec:train}
Consider a training that with input $\vx^*$ and output $\vy^*$,
find a set of parameters $\vp_x$ that satisfy $\vy^* = f(\vx^*, \vp_x)$.
In traditional machine learning, we define a loss $\mathcal{L} = {\rm dist}(\vy^*, f(\vx^*, \vp_x))$ and minimize it with gradient $\frac{\partial L}{\partial \vp_x}$. This works only when the target function is locally differentiable.

Here we provide an alternative by making use of reversibility.
We construct a reversible program $\vy, \vp_y =  f_r(\vx, \vp_x)$, where $\vp_x$ and $\vp_y$ are ``parameter'' spaces on the input side and output side.
The algorithm can be summarized as

\begin{algorithm}[H]
    \KwResult{$\vp_x$}
    Initialize $\vx$ to $\vx^*$, parameter space $\vp_x$ to random.\\
    \eIf{$\vp_y$ is \texttt{null}}{
        $\vx, \vp_x = f_r^{-1}(\vy^*)$\\
    }{
        $\vy, \vp_y= f_r(\vx, \vp_x)$\\
        \While{$\vy \not\approx \vy^*$}{
            $\vy = \vy^*$\\
            $\vx, \vp_x = f_r^{-1}(\vy, \vp_y)$.\\
            $\vx = \vx^*$\\
            $\vy, \vp_y= f_r(\vx, \vp_x)$
        }
    }
    \caption{Learn by consistency}\label{algo:train}
\end{algorithm}

Here, $\parameter(\cdot)$ is a function for taking the parameter space.
This algorithm utilizes the self-consistency relation
\begin{equation}\label{eq:selfconsistent}
    \vp_x^* = \parameter(f_r^{-1}(\vy^*, \parameter(f_r(\vx^*, \vp^*_x)))),
\end{equation}

Similar idea of training by consistency is used in self-consistent meanfield theory~\cite{Michael2003} in physics.
Finding the self-consistent relation is crucial to a self-consistency based training. Here, the reversibility provides a natural self-consistency relation.
However, it is not a silver bullet, let's consider the following example

\begin{minipage}{.44\textwidth}
\begin{lstlisting}[basicstyle=\small\ttfamily,columns=fullflexible]
@i function f1(y!, x, p!)
    p! += identity(x)
    y! -= exp(x)
    y! += exp(p!)
end

@i function f2(y!, x!, p!)
    p! += identity(x!)
    y! -= exp(x!)
    x! -= log(-y!)
    y! += exp(p!)
end

function train(f)
    loss = Float64[]
    p = 1.6
    for i=1:100
        y!, x = 0.0, 0.3
        @instr f(y!, x, p)
        push!(loss, y!)
        y! = 1.0
        @instr (~f)(y!, x, p)
    end
    loss
end
\end{lstlisting}
\end{minipage}

Functions \texttt{f1} and \texttt{f2} computes $f(x, p) = e^{(p+x)} - e^x$ and stores the output in a new memory \texttt{y!}.
The only difference is \texttt{f2} uncomputes $x$ arithmetically.
The task of training is to find a $p$ that make the output value equal to target value $1$.
After $100$ steps, \texttt{f2} runs into the fixed point with $x$ equal to $1$ upto machine precision.
However, parameters in \texttt{f1} does change at all.
The training of \texttt{f1} fails because this function actually computes $\texttt{f1}(y, x, p) = y + e^{(p+x)} - e^{x}, x, x+p$, where the training parameter $p$ is completely determined by the parameter space on the output side $x \cup x+p$. As a result, shifting $y$ directly is the only approach to satisfy the consistency relation. On the other side, $\texttt{f2}(y, x, p) = y + e^{(p+x)} - e^x, \tilde{0}, x+p$, the output parameters $\tilde{0} \cup x+p$ can not uniquely determine input parameters $p$ and $x$. Here, we use $\tilde{0}$ to denote the zero with rounding error.

\begin{figure}
    \centerline{\includegraphics[width=0.9\columnwidth,trim={0 0.3cm 0 0},clip]{images/fig1.pdf}}
    \caption{The output value \texttt{y!} as a function of self-consistent training step.}\label{fig:invtrain}
\end{figure}

By viewing $\vx$ and parameters in $\vp_x$ as variables, we can study the trainability from the information perspective.
\begin{theorem}
    Only if the the conditional entropy $S(\vy|\vp_y)$ is nonzero, algorithm \ref{algo:train} is trainable.
\end{theorem}
\begin{proof}
The above example reveals a fact that training impossible when output parameters completely determines input parameters (or $S(\vp_x | \vp_y) = 0$).
\begin{align}
    \begin{split}
        S(\vp_x | \vp_y) &= S(\vp_x \cup \vp_y) - S(\vp_y)\\
        &\leq S\left((\vp_x \cup \vx) \cup \vp_y \right) - S(\vp_y),\\
        &\leq S\left((\vp_y \cup \vy) \cup \vp_y\right) - S(\vp_y),\\
    &\leq S(\vy|\vp_y).
    \end{split}
\end{align}
The third line uses the bijectivity $S(\vx \cup \vp_x) = S(\vy \cup \vp_y)$.
This inequality shows that when the parameter space on the output side satisfies $S(\vy | \vp_y) = 0$, i.e. contains all information to determine the output field, the input parameters are also completely determined by this parameter space, hence traning can not work.
\end{proof}
In the above example, it corresponds to the case $S\left(e^{(x+y)-e^x} | x \cup x + y\right) = 0$ in $f1$.
The solution is to remove the information redundancy in output parameter space through uncomputing as shown in $f2$.

\section{Functions used in the main text}\label{app:functions}

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
"""
    dot(out!, v1, v2)

dot product.
"""
@i function dot(out!, v1::Vector{T}, v2) where T
    for i = 1:length(v1)
        out! += v1[i]'*v2[i]
    end
end

"""
    norm2(out!, vec)

squared norm.
"""
@i function norm2(out!, vec::Vector{T}) where T
    anc1 ← zero(T)
    for i = 1:length(vec)
        anc1 += identity(vec[i]')
        out! += anc1*vec[i]
        anc1 -= identity(vec[i]')
    end
end
\end{lstlisting}
\end{minipage}

In \texttt{norm2}, we copied \texttt{vec[i]\textquotesingle} to \texttt{anc1} to avoid the same variable appear twice in the argument list of $\oplus(*)$, where the prime represents the adjoint dataview.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
"""
    mean(out!, x)

Get the mean value of `x`.
"""
@i function mean(out!, x)
    anc ← zero(out!)
    for i=1:length(x)
        anc += identity(x[i])
    end
    out! += anc / length(x)
    anc -= out! * length(x)
end
\end{lstlisting}
\end{minipage}

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
"""
    var_and_mean(var!, mean!, v)

Get the variance and mean of `x`.
"""
@i function var_and_mean(var!, mean!, v::AbstractVector{T}) where T
    cum ← zero(var!)
    n ← length(v)-1
    mean(mean!, v)
    for i=1:length(v)
        v[i] -= identity(mean!)
        cum += v[i] ^ 2
        v[i] += identity(mean!)
    end
    var! += cum / n
    cum -= var! * n
end
\end{lstlisting}
\end{minipage}

\section{CUDA compitibility}\label{app:cuda}
It is interesting to see how NiLang combines with other parts of Julia ecosystem like CUDAnative~\cite{Besard2018}.
One can write a reversible kernel like

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
using CuArrays, CUDAnative, GPUArrays
using NiLang, NiLang.AD

@i @inline function swap_kernel(state::AbstractVector{T}, mask1, mask2) where T
    @invcheckoff b ← (blockIdx().x-1) * blockDim().x + threadIdx().x
    @invcheckoff if (b < length(state), ~)
        if (b&mask1==0 && b&mask2==mask2, ~)
            NiLang.SWAP(state[b+1], state[b ⊻ (mask1|mask2) + 1])
        end
    end
end
\end{lstlisting}
\end{minipage}

This kernel function simulates the SWAP gate in quantum computing.
Here, we use the macro \texttt{@invcheckoff} to remove the reversibility checks.
It is nessesary because the possible error thrown in a kernel function can not be handled properly on a CUDA thread.
One can lannch this kernel function to GPUs with a single macro \texttt{@cuda} as show in the following using case.

\begin{minipage}{.44\textwidth}
\begin{lstlisting}
julia> @i function instruct!(state::CuVector, gate::Val{:SWAP}, locs::Tuple{Int,Int})
           mask1 ← 1 << (tget(locs, 1)-1)
           mask2 ← 1 << (tget(locs, 2)-1)
           XY ← GPUArrays.thread_blocks_heuristic(length(state))
           @cuda threads=tget(XY,1) blocks=tget(XY,2) swap_kernel(state, mask1, mask2)
       end

julia> instruct!(CuArray(randn(8)), Val(:SWAP), (1,3))[1]
8-element CuArray{Float64,1,Nothing}:
 -0.06956048379200473
 -0.6464176838567472
 -0.06523362834285944
 -0.7314356941903547
  1.512329204247244
  0.9773772766637732
  1.6473223915215722
 -1.0631789613639087
\end{lstlisting}
\end{minipage}



\end{document}
