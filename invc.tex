%\documentclass[a4paper,superscriptaddress,11pt]{quantumarticle}
%\documentclass[aps,twocolumn,longbibliography,english,superscriptaddress]{revtex4-1}
\documentclass{article}
\usepackage{neurips_2020}
%\documentclass[a4paper,superscriptaddress,11pt]{article}
\pdfoutput=1
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{quoting}
\usepackage{upquote}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{wrapfig}
%\usepackage{caption}
%\usepackage[plain]{algorithm}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{rotating}
%\usepackage{cite}
\usepackage{booktabs}
%\usepackage{unicode-math}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithm
%\usepackage{algpseudocode}% http://ctan.org/pkg/algpseudocode
\usepackage{xcolor}% http://ctan.org/pkg/xcolor
\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother
\usepackage{bbm}
\usepackage{jlcode}
\usepackage{graphicx}
\usepackage{amsmath,color,amsthm}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage[normalem]{ulem}
\usepackage{indentfirst}
\usepackage{txfonts}
\usepackage[epsilon, tsrm, altpo]{backnaur}

\newcommand{\listingcaption}[1]%
{%
\refstepcounter{lstlisting}\hfill%
Listing \thelstlisting: #1\hfill%\hfill%
}%
\newcolumntype{b}{X}
\newcolumntype{s}{>{\hsize=.7\hsize}X}
\usepackage{listings}
\lstset{
    language=Julia,
    basicstyle=\ttfamily\scriptsize,
    numberstyle=\scriptsize,
    % numbers=left,
    backgroundcolor=\color{gray!7},
    %backgroundcolor=\color{white},
    %frame=single,
    xleftmargin=2em,
    tabsize=2,
    rulecolor=\color{black!15},
    %title=\lstname,
    escapeinside={(*}{*)},
    breaklines=true,
    %breakatwhitespace=true,
    %framextopmargin=2pt,
    %framexbottommargin=2pt,
    frame=bt,
    extendedchars=true,
    inputencoding=utf8,
    columns=fullflexible,
    %escapeinside={(*@}{@*)},
}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=1000
\hbadness=1000

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

%Journal reference.  Comma sets off: name, vol, page, year
\def\journal #1, #2, #3, 1#4#5#6{{\sl #1~}{\bf #2}, #3 (1#4#5#6) }
\def\pr{\journal Phys. Rev., }
\def\prb{\journal Phys. Rev. B, }
\def\prl{\journal Phys. Rev. Lett., }
\def\pl{\journal Phys. Lett., }
%\def\np{\journal Nucl. Phys., }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\usepackage{CJK}
%\usepackage[colorlinks, citecolor=blue]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}

%%%%%% Shortcut related
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\newcommand{\out}{{\vx^L}}
\newcommand{\inp}{{\vx^0}}
\newcommand{\cquad}{{{ }_{\quad}}}
\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\minuseq}{\mathrel{-}=}
\newcommand{\vx}{{\mathbf{x}}}
\newcommand{\vg}{{\mathbf{g}}}
\newcommand{\vp}{{\mathbf{p}}}
\newcommand{\vy}{{\mathbf{y}}}
\newcommand{\Var}{{\mathrm{Var}}}
\newcommand{\Mean}{{\mathrm{E}}}
\newcommand{\vvalue}{{\texttt{value}}}
\newcommand{\grad}{{\texttt{grad}}}
\newcommand{\parameter}{{\texttt{parameter}}}
%%%%%% Convention related
\newcommand{\SWAP}{{\rm SWAP}}
\newcommand{\CNOT}{{\rm CNOT}}
\newcommand{\X}{{\rm X}}
\renewcommand{\H}{{\rm H}}
\newcommand{\Rx}{{\rm Rx}}
\renewcommand{\v}[1]{{\bf #1}}
\newcommand{\dataset}{{\mathcal{D}}}
\newcommand{\wfunc}{{\psi}}
\newcommand{\SU}{{\rm SU}}
\newcommand{\UU}{{\rm U}}
\newcommand{\thetav}{{\boldsymbol{\theta}}}
\newcommand{\gammav}{{\boldsymbol{\gamma}}}
\newcommand{\thetai}{{\theta^\alpha_l}}
\newcommand{\Expect}{{\mathbb{E}}}
\newcommand{\Tr}{{\rm Tr}}
\newcommand{\etc}{{\it etc~}}
\newcommand{\etal}{{\it etal~}}
\newcommand{\xset}{\mathbf{X}}
\newcommand{\fl}{\texttt{fl}}
\newcommand{\pdata}{\mathbf{\pi}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\epdata}{\mathbf{\hat{\pi}}}
\newcommand{\gammaset}{\boldsymbol{\Gamma}}
\newcommand{\ei}{{\mathbf{e}_l^\alpha}}
\newcommand{\vtheta}{{\boldsymbol{\theta}}}
\newcommand{\sigmag}{{\nu}}
\newcommand{\sigmai}[2]{{\sigma^{#2}_{#1}}}
\newcommand{\qi}[1]{{q^{\alpha_{#1}}_{#1}}}
\newcommand{\BAS}{Bars-and-Stripes}
\newcommand{\circled}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}

\newcommand{\qexpect}[1]{{\left\langle #1\right\rangle}}
\newcommand{\expect}[2]{{\mathop{\mathbb{E}}\limits_{\substack{#2}}\left[#1\right]}}
\newcommand{\var}[2]{{\mathop{\mathrm{Var}}\limits_{\substack{#2}}\left(#1\right)}}
\newcommand{\pshift}[1]{{p_{\thetav+#1}}}
\newcommand{\upcite}[1]{\textsuperscript{\cite{#1}}}
\newcommand{\Eq}[1]{Eq.~(\ref{#1})}
\newcommand{\Fig}[1]{Fig.~\ref{#1}}
\newcommand{\Lst}[1]{Listing.~\ref{#1}}
\newcommand{\Tbl}[1]{Table~\ref{#1}}
\newcommand{\Sec}[1]{Sec.~\ref{#1}}
\newcommand{\bra}[1]{\mbox{$\left\langle #1 \right|$}}
\newcommand{\ket}[1]{\mbox{$\left| #1 \right\rangle$}}
\newcommand{\braket}[2]{\mbox{$\left\langle #1 | #2 \right\rangle$}}
\newcommand{\tr}[1]{\mathrm{tr}\mbox{$\left[ #1\right]$}}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

%%%%%% Comment related
\newcommand{\red}[1]{[{\bf  \color{red}{LW: #1}}]}
\newcommand{\xred}[1]{[{\bf  \color{red}{\sout{LW: #1}}}]}
\newcommand{\blue}[1]{[{\bf  \color{blue}{JG: #1}}]}
\newcommand{\violet}[1]{[{\bf  \color{violet}{MLS: #1}}]}
\newcommand{\green}[1]{[{\bf  \color{green}{TZ: #1}}]}
\newcommand{\xgreen}[1]{[{\bf  \color{green}{\sout{TZ: #1}}}]}
\newcommand{\xblue}[1]{[{\bf  \color{blue}{\sout{JG: #1}}}]}
\newcommand{\material}[1]{\iffalse[{\bf  \color{cyan}{Material: #1}}]\fi}
\newcommand{\orange}[1]{\iffalse[{\bf  \color{orange}{Jo: #1}}]\fi}

\newtheorem{theorem}{\textit{Theorem}}
\theoremstyle{definition}\newtheorem{definition}{\textit{Definition}}

\makeatother

\begin{document}
\title{Differentiate Everything with a Reversible Domain-Specific Language}

\author{Jin-Guo Liu\\
Institute of Physics, Chinese Academy of Sciences,\\Beijing 100190, China\\
\texttt{cacate0129@iphy.ac.cn}\\
\And
Taine Zhao\\
Department of Computer Science, University of Tsukuba\\
\texttt{thaut@logic.cs.tsukuba.ac.jp}\\
}
%\author{Lei Wang}
%\email{wanglei@iphy.ac.cn}
%\affiliation{Institute of Physics, Chinese Academy of Sciences, Beijing 100190, China}
%\affiliation{CAS Center for Excellence in Topological Quantum Computation, University of Chinese Academy of Sciences, Beijing 100190, China}
%\affiliation{Songshan Lake Materials Laboratory, Dongguan, Guangdong 523808, China}

%\author{Jin-Guo Liu}
%\email{cacate0129@iphy.ac.cn}
%\affiliation{Institute of Physics, Chinese Academy of Sciences, Beijing 100190, China}

%\author{Taine Zhao}
%\affiliation{Department of Computer Science, University of Tsukuba}

\maketitle

\begin{abstract}
Reverse-mode automatic differentiation (AD) suffers from the issue of having too much space overhead to trace back intermediate computational states for backpropagation.
The traditional method to trace back states is called checkpointing that stores intermediate states into a global stack and restore state through either stack pop or re-computing.
The overhead of stack manipulations and re-computing makes the general purposed (not tensor-based) AD engines unable to meet many industrial needs.
Instead of checkpointing, we propose to use reverse computing to trace back states by designing and implementing a reversible programming eDSL, where a program can be executed bi-directionally without implicit stack operations. The absence of implicit stack operations makes the program compatible with existing compiler features, including utilizing existing optimization passes and compiling the code as GPU kernels.
We implement AD for sparse matrix operations and some machine learning applications to show that the performance of our framework has state-of-the-art performance.
\end{abstract}

%\begin{multicols}{2}
\section{Introduction}\label{sec:intro}
    Most popular autmatic differetiation (AD) packages in the market, such as TensorFlow~\cite{Tensorflow2015}, Pytorch~\cite{Paszke2017} and Flux~\cite{Innes2018a} implements reverse mode AD at the tensor level to meet the need of machine learning. Later, People in the scientific computing domain also find the powerfulness of these AD tools, people use these them to solve scientific problems such as seismic inversion~\cite{Zhu2020}, variational quantum circuits simulation~\cite{Bergholm2018} and variational tensor network simulation~\cite{Liao2019,Roberts2019}. To meet the diversed need in these applications, one often needs to add manually defined backward rules, for example
    \begin{enumerate}
        \item In order to differentiate sparse matrix operations for Hamiltonian engineering~\cite{Xie2020}, we need to define backward rules for sparse matrix operations and dominant eigensolvers~\cite{Golub2012},
        \item In tensor network algorithms to study the phase transition problem~\cite{Golub2012,Liao2019,Seeger2017,Wan2019,Hubig2019}, one needs defining backward rules for singular value decomposition (SVD) function and QR decomposition.
    \end{enumerate}
    %Scientists have put lots of effort to derive new backward rules for these needs.
    To avoid defining backward rules manually, one can also use a general purposed AD (GP-AD) packages like Tapenade~\cite{Hascoet2013}, OpenAD~\cite{Utke2008} and Zygote~\cite{Innes2018, Innes2019} to differentiate a general program. These tools has been used in non-tensor based applications such as bundle adjustment~\cite{Shen2018} and earth system simulation~\cite{Forget2015}.
They read the source code from a user and generate code to compute the gradients. However, these packages have their own limitations too. In many practical applications, differentiating a program might do billions of computations. Frequent caching of data slows down the program significantly, while the memory usage will become a bottleneck as well. Moreover, implicit caching is not compatible with differentiating GPU kernels.

    These needs call for a GP-AD framework that does not cache for users automatically. Hence we propose to implement the reverse mode AD on a reversible (domain-specific) programming language~\cite{Perumalla2013,Frank2017}. So that the intermediate states of a program can be traced backward with no extra effort.
In a reversible languages, the memory allocation and deallocation are explicit, with which flexible time-space tradeoff is available.
Meanwhile, one can also utilize the reversibility to reverse the program without space overhead, which is proven to significantly decrease the memory usage in unitary recurrent neural networks~\cite{MacKay2018}, normalizing flow~\cite{Dinh2014}, hyperparameter learning~\cite{Maclaurin2015} and residual neural networks~\cite{Behrmann2018}. Using reversible programming language makes all these happen naturally without extra framework designs.

 There have been many prototypes of reversible languages like Janus~\cite{Lutz1986}, R (not the popular one)~\cite{Frank1997}, Erlang~\cite{Lanese2018} and object-oriented ROOPL~\cite{Haulund2017}. % These languages have reversible control flow that allows users to input an additional postcondition in control flows to help programs run backward.
    In the past, the primary motivation to study reversible programming is to support reversible computing devices~\cite{Frank1999} like adiabatic complementary metal–oxide–semiconductor (CMOS)~\cite{Koller1992}, molecular mechanical computing system~\cite{Merkle2018} and superconducting system~\cite{Likharev1977,Semenov2003}, where a reversible computing device is more energy-efficient by the Landauer's principle~\cite{Landauer1961}.
    The above reversible programming languages are well defined so that can be compiled to future hardwares. However, the drawback is they can hardly be used in real scientific programming, since most of them do not have basic elements like floating point numbers, arrays and complex numbers. This motivates us to build a new embeded domain specific language (eDSL) in Julia~\cite{Bezanson2012,Bezanson2017}.
%After decades of efforts, reversible computing devices are very close to providing productivity now. As an example, adiabatic CMOS can be a better choice in a spacecraft~\cite{Hanninen2014, Debenedictis2017}, where energy is more valuable than device itself.
%Reversible programming is interesting to software engineers too, because it is a powerful tool to schedule asynchronious events~\cite{Jefferson1985} and debug a program bidirectionally~\cite{Boothe2000}.
%However, the field of classical reversible computing faces the issue of having not enough funding in recent decade~\cite{Frank2017}. As a result, not many people in machine learning are familiar with some marvelous designs in reversible computing. People have not connected it with automatic differentiation seriously, even though they share so many similarities.

%This paper aims to break the information barrier between the machine learning community and the reversible programming community and provide yet another strong motivation to develop reversible programming.
    In this paper, we first introduce the language design of a reversible programming language and introduce our reversible eDSL NiLang in \Sec{sec:lang}.
    In \Sec{sec:bp}, we explain the implementation of automatic differentiation in this eDSL.
    In \Sec{sec:benchmark}, we benchmark the performance of NiLang with other AD packages and explain why reversible programming AD is fast.
    In the appendix, we show the detailed language design of NiLang, show some examples used in the benchmark, discuss several important issues including the time-space  tradeoff, reversible instructions and hardware, and finally, an outlook to some open problems to be solved.


\section{Language design}\label{sec:lang}

In the introduction, we have introduced several reversible programming languages. These languages lacks essential components for scientific programming like arrays and complex numbers, while none of them can be embeded in other machine learning frameworks easily.
Hence we developped an embedded domain-specific language (eDSL) NiLang on top of the host language language Julia~\cite{Bezanson2012,Bezanson2017}.
Julia is a popular language for scientific programming and machine learning. We choose Julia mainly for speed. Julia is a language with high abstraction, however, its clever design of type inference and just in time compiling make it has a C like speed.
Also, it has rich features for meta-programming, and the package for pattern matching \href{https://github.com/thautwarm/MLStyle.jl}{MLStyle} allows us to define an eDSL compiler in less than 2000 lines.
Comparing with a regular reversible programming language, NiLang features array operations, rich number systems including floating point number, complex number, fixed point number and logarithmic number.
Besides the above nice features, it also has some "bad" features to meet the practical needs. For example, it views the floating point $\mathrel{+}$ and $\mathrel{-}$ operations as reversible. It also allows user to extend instruction sets and sometimes inserting external statements. These features are not compatible with future reversible hardwares.
By the time of writting, the version of NiLang is v0.7.2.

\subsection{Reversible functions and arithmetic instructions}
    Mathematically, a irreversilbe mapping \texttt{y = f(args...)} can be trivially transfromed to its reversible form \texttt{y += f(args...)} or \texttt{y $\veebar$= f(args...)} ($\veebar$ is the bitwise \texttt{XOR}), where \texttt{y} is a pre-emptied variable. But in numeric computing with finite precision, this is not the whole story, because the reversibility of arithmetic instruction is closely related to the number system.
    For integer and fixed point number system, \texttt{y += f(args...)} and \texttt{y -= f(args...)} are rigorously reversible. For logarithmic number system and tropical number system~\cite{Speyer2009}, \texttt{y *= f(args...)} and \texttt{y /= f(args...)} as reversible (not introducing the zero element). While for floating point numbers, none of the above operations are regorously reversible. However, for convenience, we ignore the rounding errors in floating point \texttt{+} and \texttt{-} operations and treat them on equal footing with fixed point numbers in the following discussion.
Besides the above operations, \texttt{SWAP} operation that exchanges the values of two variables is also widely used in reversible programming.

The following code defines a reversible multiplier.

\begin{minipage}{.88\columnwidth}
\begin{lstlisting}[mathescape=true,caption={A reversible mutiplier},label={lst:mutiplier}]
julia> using NiLang

julia> @i function multiplier(y!::Real, a::Real, b::Real)
           y! += a * b
       end

julia> multiplier(2, 3, 5)
(17, 3, 5)

julia> (~multiplier)(17, 3, 5)
(2, 3, 5)
\end{lstlisting}
\end{minipage}

Macro \texttt{@i} generates two functions that are reversible to each other \texttt{multiplier} and \texttt{$\sim$multiplier}, each defines a mapping $\mathbb{R}^3 \rightarrow \mathbb{R}^3$. The $!$ after a symbol is a part of the name, as a convension to indicate that this variable is changed.

\subsection{Reversible memory management}
    A distinct feature of reversible memory management is that the content of a variable must be known when it is deallocated.
    We denote the allocation of a zero emptied memory as \texttt{x $\leftarrow$ 0}, and the corresponding deallocation as \texttt{x $\rightarrow$ 0}.
    A variable can also be pushed to a stack and used later with a pop statement.
    If a variable is allocated and deallocated in a local scope, we call it an ancilla.
    %A reversible stack is different in that it zero-clears the variable after pushing and presupposes that the variable being zero-cleared before popping.
%Knowing the contents in the memory when deallocating is not easy. Hence Charles H. Bennett introduced the famous compute-copy-uncompute paradigm~\cite{Bennett1973} for reversible programming.

\begin{minipage}{.45\columnwidth}
\begin{lstlisting}[mathescape=true,caption={Reversible complex valued log function $y\mathrel{+}=\log(|x|) + i{\rm Arg}(x)$.},label={lst:complex}]
@i @inline function (:+=)(log)(y!::Complex{T}, x::Complex{T}) where T
    n ← zero(T)
    n += abs(x)

    y!.re += log(n)
    y!.im += angle(x)

    n -= abs(x)
    n → zero(T)
end
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\columnwidth}
    \begin{lstlisting}[mathescape=true,caption={Compute-copy-uncompute version of \Lst{lst:complex}},label={lst:complex2}]
@i @inline function (:+=)(log)(y!::Complex{T}, x::Complex{T}) where T
    @routine begin
        n ← zero(T)
        n += abs(x)
    end
    y!.re += log(n)
    y!.im += angle(x)
    ~@routine
end
\end{lstlisting}
\end{minipage}

\Lst{lst:complex} defines the complex valued accumulative log function.
The macro \texttt{@inline} tells the compiler that this function can be inlined. One can input ``$\leftarrow$'' and ``$\rightarrow$'' by typing ``$\backslash$leftarrow[TAB KEY]'' and ``$\backslash$rightarrow[TAB KEY]'' respectively in a Julia editor or REPL.
NiLang does not have immutable structs, so that the real part \texttt{y!.re} and imaginary \texttt{y!.im} of a complex number can be changed directly.
It is easy to verify that the bottom two lines in the function body are the reverse of the top two lines. We call the bottom two lines \textit{uncomputes} the top two lines.
The motivation is to zero clear the contents in ancilla \texttt{n} so that it can be deallocated correctly.
\textit{Compute-copy-uncompute} is a useful design pattern in reversible programming so that we created a pair of macros \texttt{@routine} and \texttt{$\sim$@routine} for it. One can rewrite the above function as in \Lst{lst:complex2}.

\subsection{Reversible control flows}
One can define reversible \texttt{if}, \texttt{for} and \texttt{while} statements in a reversible program.
\Fig{fig:controlflow} (a) shows the flow chart of executing the reversible \texttt{if} statement. There are two condition expressions in this chart, a precondition and a postcondition. The precondition decides which branch to enter in the forward execution. After executing the specific branch, the program checks the consistency between precondition and postcondition to make sure they are consistent. To reverse this statement, one can exchange the precondition and postcondition, and reverse the expressions in both branches.
\Fig{fig:controlflow} (b) shows the flow chart of the reversible \texttt{while} statement. There are also two conditions expressions. Before executing the condition expressions, the program preassumes the postcondition is false.
After each iteration, the program asserts the postcondition to be true. To reverse this statement, one can exchange the precondition and postcondition, and reverse the body statements.
The reversible \texttt{for} statement is similar to the irreversible one except that after execution, the program will assert the iterator to be unchanged. To reverse this statement, one can exchange \texttt{start} and \texttt{stop} and inverse the sign of \texttt{step}.
\begin{figure}
    \centerline{\includegraphics[width=0.9\columnwidth,trim={0 0cm 0 0cm},clip]{controlflow_v2.pdf}}
    \caption{The flow chart for reversible (a) \texttt{if} statement and (b) \texttt{while} statement. ``pre'' and ``post'' represents precondition and postconditions respectively.}\label{fig:controlflow}
\end{figure}

The following code computes the Fobonacci number recursively and reversibly.

\begin{minipage}{.88\columnwidth}
\begin{lstlisting}[mathescape=true,caption={Computing Fibonacci number recursibly and reversibly.},label={lst:fib}]
@i function rrfib(out!, n)
    @invcheckoff if (n >= 1, ~)
        counter ← 0
        counter += n
        while (counter > 1, counter!=n)
            rrfib(out!, counter-1)
            counter -= 2
        end
        counter -= n % 2
        counter → 0
    end
    out! += 1
end
\end{lstlisting}
\end{minipage}

Here, \texttt{out!} is an integer initialized to \texttt{0} for storing outputs.
The preconditions and postconditions are wrapped into a tuple. In the \texttt{if} statement, the postcondition is the same as the precondition, hence we omit the postcondition by inserting a "\texttt{$\sim$}" in the second field as a placeholder.
In the while statement, the postcondition is true only for the initial loop.
Once code is proven correct, one can turn off the reversibility check by adding \texttt{@invcheckoff} before a statement.
This will remove the reversibility check and make the code faster and compatible with GPU kernels (kernel functions can not handle exceptions).

\subsection{Reverse computing is not checkpointing}\label{sec:timespace}
There are two approaches to trace back intermediate states in reversible computing, one is checkpointing and another is reverse computing.
When we talk about reversible programming languages, we always refers to those implement reverse computing. 
The physical origin of why people prefer recovering information through reverse computing lies in how reversible computing devices saves energy.
Taking adiabatic CMOS as an example, its logic unit can be charged up and charged down (signal recovery).
To recover energy in the charging down phase, the output signal must be the same as when it was charged up.
This brings challenges to circuit archetecture designs.
Naively, one can connect the output of a logic unit to the input of the next one directly in a cascade fashion.
While all logic units does not charged down until the computation ends.
In cascade stage $n$, the activity factor scales as $1/n$ and the circuit performance suffers accordingly.~\cite{Athas1994}
Designs considered as more practical, such as SCRL and 2LAL and recent S2LAL use pipeline structure.
Where basic instructions are reversible so that signals can be recovered through uncomputing.
To compile code to adiabatic CMOS, a programming language must implement reverse computing.

In the most straightforward g-segment tradeoff scheme~\cite{Bennett1989,Levine1990,Perumalla2013}, an RTM model has either a space overhead that is proportional to computing time $T$ or a computational overhead that sometimes can be exponential to the program size comparing with an irreversible counterpart.
This is similar to the checkpointing~\cite{Griewank2008, Chen2016} that widely used in many machine learning frameworks.
Both reverse computing and checkpointing can make a program reversible.
Checkpointing takes snapshots of the computational state pior to current stage so that all intermediate results can be recomputed.
While reverse computing restores the state from the inverse direction and can utilize reversibility to avoid unnessesary allocations.
Utilizing reversibility is especially important at the lower level design, which can be seen from the connection between reversibility and adiabatic logic circuit design.
Checkpointing shares the same spirit with the cascade layout~\cite{Hall1992} for connecting adiabatic logic units.
The cascade layout is believe not practical because the very first input (the allocated memory at the checkpoint) must remain valid when the last output is allowed to go invalid. For $n$ level cascading, the activity factor for each stage will descrease as $1/n$, resulting into poor circuit performance.
On the other side, the reverse computing correspondence pipeline layout~\cite{Athas1994} can restore the input signals by running inverse operation of circuit blocks, which has been widely used mordern reversible circuit design~\cite{Anantharam2004}.
This is the underlying reason why reversible languages do not use checkpointing.
It is a solid proof from the practise that it is the reversible programming rather than the checkpointing that can differentiate the whole language from the machine instruction level.
When there is not time overhead, both have a space overhead propotional to time.
When a polynomial overhead in time is allowed, reversible computing has a minimum space overhead of $O(S\log(T))$  [Robert Y. Levine, 1990]. While for checkpointing, there can be no space overhead. One can just recompute from beginning to obtain any intermediate state.

Reverse computing shows advantage in handling effective codes with mutable structures and arrays.
For example the affine transformation can be implemented without any overhead.
\begin{minipage}{.88\columnwidth}
\begin{lstlisting}[mathescape=true,caption={Affine transformation without extra memory cost.},label={lst:affine}]
@i function i_affine!(y!::AbstractVector{T}, W::AbstractMatrix{T}, b::AbstractVector{T}, x::AbstractVector{T}) where T
    @safe @assert size(W) == (length(y!), length(x)) && length(b) == length(y!)
    @invcheckoff for j=1:size(W, 2)
        for i=1:size(W, 1)
            @inbounds y![i] += W[i,j]*x[j]
        end
    end
    @invcheckoff for i=1:size(W, 1)
        @inbounds y![i] += b[i]
    end
end
\end{lstlisting}
\end{minipage}

Reverse computing also provides a path to utilize reversibility.
It can differentiate the inplace version of unitary matrix multiplication, which can be used to implement memory efficient recurrent neural networks ~\cite{Jing2016}.

\begin{minipage}{.88\columnwidth}
\begin{lstlisting}[mathescape=true,caption={Two level decomposition of a unitary matrix.},label={lst:affine}]
@i function i_umm!(x!::AbstractArray, θ)
    M ← size(x!, 1)
    N ← size(x!, 2)
    k ← 0
    @safe @assert length(θ) == M*(M-1)/2
    for l = 1:N
        for j=1:M
            for i=M-1:-1:j
                INC(k)
                ROT(x![i,l], x![i+1,l], θ[k])
            end
        end
    end
    k → length(θ)
end
\end{lstlisting}
\end{minipage}

Last but not least, it encourages users to code in a memory friendly style.
%Reverse computing shows advantage in time-space tradeoff not only for being able to make use of reversibility,
%but also for encouraging people to code with reversible thinking.
Reversible programming does not allocate automatically for people, so that the programmer need to think how to make the program reversible and cache friendly.
For example, to compute the power of a positive fixed point number and an integer, one can easily write irreversible code as in \Lst{lst:power1}

\begin{minipage}{.45\columnwidth}
\begin{lstlisting}[mathescape=true,caption={A regular power function.},label={lst:power1}]
function mypower(x::T, n) where T
    y = one(T)
    for i=1:n
        y *= x
    end
    return y
end
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\columnwidth}
\begin{lstlisting}[mathescape=true,caption={A reversible power function.},label={lst:power2}]
@i function mypower(out, x::T, n) where T
    if (x != 0, ~)
        @routine begin
            ly ← one(ULogarithmic{T})
            lx ← one(ULogarithmic{T})
            lx *= convert(x)
            for i=1:n
                ly *= x
            end
        end
        out += convert(ly)
        ~@routine
    end
end
\end{lstlisting}
\end{minipage}

Since fixed point number is not reversible under multiplication, the regular power with checkpointing would require checkpointing inside a loop, which will cause bad performance. With reversible thinking, we can convert the fixed point number to logarithmic numbers for computing as shown in \Lst{lst:power2}. One can compute the output without sacrificing reversibility. The algorithm to convert a regular fixed point number to a logarithmic number is efficient~\cite{Turner2010}.
Even in cases where allocation inside the loop can not be avoided, reversible programming allows a user to preallocate a chunk outside of the loop, so that computation inside the loop can still be efficient.

\section{Reversible automatic differentiation}\label{sec:bp}

\subsection{First order gradient}\label{sec:jacobian}
If we inline all the instructions, the program would be like \Lst{lst:expand-complex}.
The automatically generated inverse program (i.e. $(y, x) \rightarrow (y-\log(x), x)$)}) is like \Lst{lst:reversed-complex}.

\begin{minipage}{.45\textwidth}
    \begin{lstlisting}[mathescape=true,caption={The expanded function body of \Lst{lst:bcast-complex}.},label={lst:expand-complex}, frame=tlrb]
N ← min(length(x), length(y!))
for i=1:N
    @routine begin
        nsq ← zero(T)
        n ← zero(T)
        nsq += x[i].re ^ 2
        nsq += x[i].im ^ 2
        n += sqrt(nsq)
    end
    y![i].re += log(n)
    y![i].im += atan(x[i].im, x[i].re)
    ~@routine
end
N → min(length(x), length(y!))
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
    \begin{lstlisting}[mathescape=true,caption={The inverse of \Lst{lst:expand-complex}.},label={lst:reversed-complex}, frame=tlrb]
N ← min(length(x), length(y!))
for i=N:-1:1
    @routine begin
        nsq ← zero(T)
        n ← zero(T)
        nsq += x[i].re ^ 2
        nsq += x[i].im ^ 2
        n += sqrt(nsq)
    end
    y![i].re -= log(n)
    y![i].im -= atan(x[i].im, x[i].re)
    ~@routine
end
N → min(length(x), length(y!))
\end{lstlisting}
\end{minipage}


To compute the adjoint of the computational process in \Lst{lst:expand-complex}, one simply insert the gradient code into its inverse in \Lst{lst:reversed-complex}.
The resulting code is show in \Lst{lst:grad-complex}, the original arithmetic instructions are highlighted with yellow background color, they now apply on the value field (\texttt{.x}) of the input value.
Along with these reversed code, we have inserted a bundle of extra code to update the gradient field (\texttt{.g}). \texttt{@zeros TYPE var1 var2...} is the macro to allocate multiple ancillas. Since these ``allocated'' variables are scalars, they do not really access the system memory. Its inverse operations starts with \texttt{$\sim$@zeros} returns zero emptied ancillas to the system.

\begin{minipage}{.88\columnwidth}
%\begin{multicols}{2}[\captionof{lstlisting}{Insert the gradient code into \Lst{lst:reversed-complex}.}]
    \listingcaption{Insert the gradient code into \Lst{lst:reversed-complex}.}\label{lst:grad-complex}
    \begin{lstlisting}[mathescape=true,label={lst:grad-complex}, multicols=2]
N ← min(length(x), length(y!))
for i=N:-1:1
   @routine begin
       nsq ← zero(GVar{T,T})
       n ← zero(GVar{T,T})

       gsqa ← zero(T)
       gsqa += x[i].re.x * 2
       x[i].re.g -= gsqa * nsq.g
       gsqa -= nsq.x * 2
       gsqa -= x[i].re.x * 2
       gsqa → zero(T)
       $\text{\colorbox{yellow}{nsq.x += x[i].re.x \textasciicircum 2}} $

       gsqb ← zero(T)
       gsqb += x[i].im.x * 2
       x[i].im.g -= gsqb * nsq.g
       gsqb -= x[i].im.x * 2
       gsqb → zero(T)
       $\text{\colorbox{yellow}{nsq.x += x[i].im.x \textasciicircum 2}} $

       @zeros T ra rb
       ra += sqrt(nsq.x)
       rb += 2 * ra
       nsq.g -= n.g / rb
       rb -= 2 * ra
       ra -= sqrt(nsq.x)
       ~@zeros T ra rb
       $\text{\colorbox{yellow}{n.x += sqrt(nsq.x)}} $
   end

   $\text{\colorbox{yellow}{y![i].re.x -= log(n.x)}} $
   n.g += y![i].re.g / n.x

   $\text{\colorbox{yellow}{y![i].im.x-=atan(x[i].im.x,x[i].re.x)}} $
   @zeros T xy2 jac_x jac_y
   xy2 += abs2(x[i].re.x)
   xy2 += abs2(x[i].im.x)
   jac_y += x[i].re.x / xy2
   jac_x += (-x[i].im.x) / xy2
   x[i].im.g += y![i].im.g * jac_y
   x[i].re.g += y![i].im.g * jac_x
   jac_x -= (-x[i].im.x) / xy2
   jac_y -= x[i].re.x / xy2
   xy2 -= abs2(x[i].im.x)
   xy2 -= abs2(x[i].re.x)
   ~@zeros T xy2 jac_x jac_y

   ~@routine
end
\end{lstlisting}
\end{minipage}


In really implementation, instead of inserting codes directly, we utilize Julia's multiple dispatch and ``insert'' the gradient code by overloading the basic instructions for the wrapper type \texttt{GVar}.
The same strategy has been used in the ForwardDiff package in Julia.
Thanks to the just in time compiling technology, the above code does not run as long as it looks. Computing the gradient takes similar time as computing the complex valued log with Julia's builtin log function alone.
One does not need to define gradient function for the inversed program in \Lst{lst:reversed-complex}, because taking inverse and computing gradients commute~\cite{Mcinerney2015}. Hence, we can simply reverse the gradient function in \Lst{lst:grad-complex}.

\subsection{Hessians}
Combining forward mode AD and reverse mode AD is a simple yet efficient way to obtain Hessians.
By wrapping the elementary type with \texttt{Dual} defined in package ForwardDiff~\cite{Revels2016} and throwing it into the gradient program defined in NiLang,
one obtains one row/column of the Hessian matrix straightforward.
We will examplify it in a benchmark in \Sec{sec:graphbench}.

\subsection{CUDA kernels}
CUDA programming is playing a significant role in high-performance computing. In Julia, one can write GPU compatible functions in native Julia language with \href{https://github.com/JuliaGPU/KernelAbstractions.jl}{KernelAbstractions}~\cite{Besard2018}.
Since NiLang does not push variables into stack automatically for users, it is safe to write differentiable GPU kernels with NiLang.
We will show this feature in the benchmarks of bundle adjustment (BA) in \Sec{sec:ba}.
Here, one should notice that the shared read in forward pass will become shared write in the backward pass, which may result in incorrect gradients. We will review this issue in the supplimentary material.

\section{Benchmarks}\label{sec:benchmark}

It is interesting to see how does our framework comparing with the state-of-the-art GP-AD frameworks, including source code transformation based Tapenade and Zygote and operator overloading based ForwardDiff and ReverseDiff.
Since most DS-AD packages like famous Tensorflow and Pytorch are not dessigned for the using cases used in our benchmarks, we do not include those package to avoid an unfair comparison.
In the following benchmarks, the CPU device is Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz, and the GPU device is Nvidia Titan V.
% the GPU time is estimated by broadcasting the gradient function on CUDA array of size $2^{17}$ and taking the average.
For NiLang benchmarks, we have turned the reversibility check off to achieve a better performance. Codes used in benchmarks could be found the in Examples section of the supplimentary material.

\subsection{Sparse matrices}\label{sec:benchsparse}
We benchmarked the call, uncall and backward propagation time used for sparse matrix dot product and matrix multiplication.
Here, we estimate the time for back propagating gradients rather than including both forward and backward, since \texttt{mul!} does not output a scalar as loss.

\begin{table}[h!]\centering
\begin{minipage}{0.8\columnwidth}
\ra{1.3}
    \scalebox{1.0}{
        \begin{tabularx}{\textwidth}{bsb}\toprule
            \textbf{}     & \texttt{dot}         & \texttt{mul!} (complex valued) \\
            \hline
            Julia-O       & 3.493e-04   & 8.005e-05\\
            NiLang-O      & 4.675e-04   & 9.332e-05\\
            \hline
            NiLang-B      & 5.821e-04   & 2.214e-04\\
            \bottomrule
        \end{tabularx}
    }
    \caption{Absolute runtimes in seconds for computing the objectives (O) and the backward pass (B) of sparse matrix operations. The matrix size is $1000 \times 1000$, and the element density is $0.05$. The total time used in computing gradient can be estimated by summing ``O'' and ``B''.
    }\label{tbl:sparse}
\end{minipage}
\end{table}

The time used for computing backward pass is approximately 1.5-3 times the Julia's native forward pass.
This is because the instruction length of differentiating basic arithmetic instructions is longer than pure computing.


\subsection{Graph embedding problem}\label{sec:graphbench}
Since one can combine ForwardDiff and NiLang to obtain Hessians,
it is interesting to see how much performance we can get in differentiating the graph embedding program. The problem definition could be found in the supplimentary material.

\begin{table}[h!]\centering
    \small
\begin{minipage}{\columnwidth}
\ra{1.3}
    \scalebox{1.0}{
        \begin{tabularx}{\textwidth}{bssssssssss}\toprule
            $k$                      & 2          & 4          & 6          & 8          & 10         \\
            \hline
            Julia-O                  & 4.477e-06  & 4.729e-06  & 4.959e-06  & 5.196e-06  & 5.567e-06  \\
            NiLang-O                 & 7.173e-06  & 7.783e-06  & 8.558e-06  & 9.212e-06  & 1.002e-05  \\
            NiLang-U                 & 7.453e-06  & 7.839e-06  & 8.464e-06  & 9.298e-06  & 1.054e-05  \\
            \hline
            NiLang-G                 & 1.509e-05  & 1.690e-05  & 1.872e-05  & 2.076e-05  & 2.266e-05  \\
            ReverseDiff-G            & 2.823e-05  & 4.582e-05  & 6.045e-05  & 7.651e-05  & 9.666e-05  \\
            ForwardDiff-G            & 1.518e-05  & 4.053e-05  & 6.732e-05  & 1.184e-04  & 1.701e-04  \\
            Zygote-G                 & 5.315e-04  & 5.570e-04  & 5.811e-04  & 6.096e-04  & 6.396e-04  \\
            \hline
            (NiLang+F)-H             & 4.528e-04  & 1.025e-03  & 1.740e-03  & 2.577e-03  & 3.558e-03  \\
            ForwardDiff-H            & 2.378e-04  & 2.380e-03  & 6.903e-03  & 1.967e-02  & 3.978e-02  \\
            (ReverseDiff+F)-H        & 1.966e-03  & 6.058e-03  & 1.225e-02  & 2.035e-02  & 3.140e-02  \\
            \bottomrule
        \end{tabularx}
    }
    \caption{Absolute times in seconds for computing the objectives (O), uncall objective (U), gradients (G) and Hessians (H) of the graph embedding program.
    $k$ is the embedding dimension, the number of parameters is $10k$.
    }\label{tbl:graphembedding}
\end{minipage}
\end{table}

In \Tbl{tbl:graphembedding}, we show the the performance of different implementations by varying the dimension $k$. The number of parameters is $10k$.
As the baseline, (a) shows the time for computing the function call. We have reversible and irreversible implementations, where the reversible program is slower than the irreversible native Julia program by a factor of $\sim2$ due to the uncomputing overhead.
The reversible program shows the advantage of obtaining gradients when the dimension $k \geq 3$. The larger the number of inputs, the more advantage it shows due to the overhead proportional to input size in forward mode AD.
The same reason applies to computing Hessians, where the combo of NiLang and ForwardDiff gives the best performance for $k \geq 3$.

\subsection{Gaussian mixture model and bundle adjustment}\label{sec:ba}

We reproduced the benchmarks for Gaussian mixture model (GMM) and bundle adjustment (BA) in ~\cite{Srajer2018} by re-writing the programs in a reversible style. We show the results in \Tbl{tbl:gmm} and \Tbl{tbl:ba}. In our new benchmarks, we also rewrite the ForwardDiff program for a fair benchmark, this explains the difference between our results and the original benchmark. The Tapenade data is obtained by executing the docker file provided by the original benchmark, which provides a baseline for comparison.

\begin{table}[h!]\centering
    \scriptsize
\begin{minipage}{\columnwidth}
\ra{1.3}
    \scalebox{1.0}{
        \begin{tabularx}{\textwidth}{bssssssss}\toprule
            \textbf{\# parameters} & 3.00e+1 & 3.30e+2 & 1.20e+3 & 3.30e+3 & 1.07e+4 & 2.15e+4 & 5.36e+4 & 4.29e+5\\
            \hline
            Julia-O       & 9.844e-03 & 1.166e-02 & 2.797e-01 & 9.745e-02 & 3.903e-02 & 7.476e-02 & 2.284e-01 & 3.593e+00  \\
            NiLang-O      & 3.655e-03 & 1.425e-02 & 1.040e-01 & 1.389e-01 & 7.388e-02 & 1.491e-01 & 4.176e-01 & 5.462e+00  \\
            Tapende-O     & 1.484e-03 & 3.747e-03 & 4.836e-02 & 3.578e-02 & 5.314e-02 & 1.069e-01 & 2.583e-01 & 2.200e+00\\
            \hline
            ForwardDiff-G & 3.551e-02 & 1.673e+00 & 4.811e+01 & 1.599e+02 & -         & -         & -         & -  \\
            NiLang-G      & 9.102e-03 & 3.709e-02 & 2.830e-01 & 3.556e-01 & 6.652e-01 & 1.449e+00 & 3.590e+00 & 3.342e+01  \\
            Tapenade-G    & 5.484e-03 & 1.434e-02 & 2.205e-01 & 1.497e-01 & 4.396e-01 & 9.588e-01 & 2.586e+00 & 2.442e+01\\
            \bottomrule
        \end{tabularx}
    }
    \caption{Absolute runtimes in seconds for computing the objective (O) and gradients (G) of GMM with 10k data points. ``-'' represents missing data due to not finishing the computing in limited time.}\label{tbl:gmm}
\end{minipage}
\end{table}

In the GMM benchmark, NiLang's objective function has overhead comparing with irreversible programs in most cases.
Except the uncomputing overhead, it is also because our naive reversible matrix-vector multiplication is much slower than the highly optimized BLAS function, where the matrix-vector multiplication is the bottleneck of the computation.
The forward mode AD suffers from too large input dimension in the large number of parameters regime.
Although ForwardDiff batches the gradient fields, the overhead proportional to input size still dominates.
The source to source AD framework Tapenade is faster than NiLang in all scales of input parameters,
but the ratio between computing the gradients and the objective function are close.
%The memory consumption of our reversible implementation is quite low.
%The peak memory is only slightly more than twice of the original program,
%where the factor 2 comes from wrapping each variable with an extra gradient field.

\begin{table}[h!]\centering
    \scriptsize
\begin{minipage}{\columnwidth}
\ra{1.3}
    \scalebox{1.0}{
        \begin{tabularx}{\textwidth}{bsssssss}\toprule
            \textbf{\# measurements} & 3.18e+4 & 2.04e+5 & 2.87e+5 & 5.64e+5 & 1.09e+6 & 4.75e+6 & 9.13e+6\\
            \hline
            Julia-O        & 2.020e-03 & 1.292e-02 & 1.812e-02 & 3.563e-02 & 6.904e-02 & 3.447e-01 & 6.671e-01\\
            NiLang-O       & 2.708e-03 & 1.757e-02 & 2.438e-02 & 4.877e-02 & 9.536e-02 & 4.170e-01 & 8.020e-01\\
            Tapenade-O     & 1.632e-03 & 1.056e-02 & 1.540e-02 & 2.927e-02 & 5.687e-02 & 2.481e-01 & 4.780e-01\\
            \hline
            ForwardDiff-J  & 6.579e-02 & 5.342e-01 & 7.369e-01 & 1.469e+00 & 2.878e+00 & 1.294e+01 & 2.648e+01\\
            NiLang-J       & 1.651e-02 & 1.182e-01 & 1.668e-01 & 3.273e-01 & 6.375e-01 & 2.785e+00 & 5.535e+00\\
            NiLang-J (GPU) & 1.354e-04 & 4.329e-04 & 5.997e-04 & 1.735e-03 & 2.861e-03 & 1.021e-02 & 2.179e-02\\
            Tapenade-J     & 1.940e-02 & 1.255e-01 & 1.769e-01 & 3.489e-01 & 6.720e-01 & 2.935e+00 & 6.027e+00\\
            \bottomrule
        \end{tabularx}
    }
    \caption{Absolute runtimes in seconds for computing the objective (O) and Jacobians (J) in bundle adjustment.}\label{tbl:ba}
\end{minipage}
\end{table}

In the BA benchmark, reverse mode AD shows slight advantage over ForwardDiff.
The bottleneck of computing this large sparse Jacobian is computing the Jacobian of a elementary function with 15 input arguments and 2 output arguments, where input space is larger than the output space.
In this instance, our reversible implementation is even faster than the source code transformation based AD framework Tapenade.
Comparing with Tapenade that inserting stack operations into the code automatically. NiLang gives users the flexibility to memory management, so that the code can be compiled to GPU.
With KernelAbstractions, we compile our reversible program to GPU with no more than 10 lines of code, which provides a >200x speed up.

%Running codes on GPU (in Julia) requires not introducing any memory allocation at the kernel level.
%NiLang provides user the flexibility to manage the memory allocation instead of using a global stack.

\section{Nitpicking NiLang}
Although there is no limitation in writing a general program in a reversible form. It is generally hard for one to get used to this programming style.
It is a chanllenge for authors of this paper to figure out the design patterns in reversible programming too.
With more and more experience, we find writting a reversible program is just as simple as writting a regular program.
\begin{quoting}
The strangeness of the reversible programming style is due mainly to our lack of experience with it. -- ~\cite{Baker1992}
\end{quoting}
The main limitation of NiLang is using floating point number might cause the accumulation of rounding errors.
A better number system for a reversible programming language might be a combination of fixed point numbers and logarithmic numbers.
Most analytic functions can be computed by Taylor explasion with constant memory and time overhead. One can see supplimentary material for an example of computing the Bessel function.

\section*{Broader Impact}
Our automatic differentiation in a reversible eDSL brings the field of reversible computing to the modern context. We believe it will be accepted by the public to meet current scientific automatic differentiation needs and aim for future energy-efficient reversible devices.
For solving practical issues, in an unpublished paper, we have successfully differentiated a spin-glass solver to find the optimal configuration on a $28\times 28$ square lattice in a reasonable time. There are also some interesting applications like normalizing flow and bundle adjustment in the example folder of \href{https://github.com/GiggleLiu/NiLang.jl}{NiLang} repository and \href{https://github.com/JuliaReverse}{JuliaReverse} organization.
For the future, energy consumption is an even more fundamental issue than computing time and memory. Current computing devices, including CPU, GPU, TPU, and NPU consume much energy, which will finally hit the "energy wall". We must get prepared for the technical evolution of reversible computing (quantum or classical), which may cost several orders less energy than current devices.

We also see some drawbacks to the current design. It requires the programmer to change to programing style rather than put effort into optimizing regular codes. It is not fully compatible with modern software stacks. Everything, including instruction sets and BLAS functions, should be redesigned to support reversible programming better. We put more potential issues and opportunities in the discussion section of the supplementary material. Solving these issues requires the participation of people from multiple fields.

\begin{ack}
Jin-Guo Liu thank Lei Wang for motivating the project with possible applications to reversible integrator, normalizing flow, and neural ODE.
Johann-Tobias Schäg for deepening the discussion about reversible programming with his mathematicians head.
Marisa Kiresame and Xiu-Zhe Luo for discussion on the implementation details of source-to-source automatic differentiation,
Shuo-Hui Li for helpful discussion on differential geometry, Tong Liu and An-Qi Chen for helpful discussion on quantum adders and multipliers, Ying-Bo Ma for correcting typos by submitting pull requests, Chris Rackauckas for helpful discussion on reversible integrator, Mike Innes for reviewing the comments about Zygote, Jun Takahashi for discussion about the graph embedding problem, Simon Byrne and Chen Zhao for helpful discussion on floating-point and logarithmic numbers.
The authors are supported by the National Natural Science Foundation of China under Grant No.~11774398, the Strategic Priority Research Program of Chinese Academy of Sciences Grant No.~XDB28000000.
\end{ack}

\bibliographystyle{apsrev4-1}
\bibliography{invc}


\end{document}
